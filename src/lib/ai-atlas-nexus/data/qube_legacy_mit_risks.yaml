taxonomies:
- id: qube-legacy-mit
  name: QUBE Legacy MIT AI Risk Repository
  description: Legacy MIT AI Risk Repository imported into QUBE AI Risk Data
risks:
- id: qube-mit-0001
  name: Diffuse creation, accountability loss
  description: Societal-scale harm can arise from AI built by a diffuse collection
    of creators, where no one is uniquely accountable for the technology's creation
    or use, as in a classic "tragedy of the commons".
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Legal risk; Reputational risk; Strategic risk
  severity: High
  likelihood: Possible
- id: qube-mit-0002
  name: Unexpected low-impact AI causes harm
  description: Harm can result from AI that was not expected to have a large impact
    at all, such as a lab leak, a surprisingly addictive open-source product, or an
    unexpected repurposing of a research prototype.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Health and safety risk; Operational risk; Reputational risk; Strategic
    risk; Technological risk; Third-party/vendor risk
  severity: High
  likelihood: Possible
- id: qube-mit-0003
  name: Intended good AI causes harm
  description: AI intended to have a large societal impact can turn out harmful by
    mistake, such as a popular product that creates problems and partially solves
    them only for its users.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Financial risk; Operational risk; Reputational risk; Strategic risk;
    Technological risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0004
  name: Willful societal harm for profit
  description: As a side effect of a primary goal like profit or influence, AI creators
    can willfully allow it to cause widespread societal harms like pollution, resource
    depletion, mental illness, misinformation, or injustice.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Environmental risk; Fraud risk; Health and safety
    risk; Legal risk; Reputational risk; Strategic risk
  severity: High
  likelihood: Possible
- id: qube-mit-0005
  name: Criminals weaponize AI for harm
  description: One or more criminal entities could create AI to intentionally inflict
    harms, such as for terrorism or combating law enforcement.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Cybersecurity risk; Fraud risk; Geopolitical risk; Health and safety
    risk; Legal risk; Strategic risk
  severity: High
  likelihood: Possible
- id: qube-mit-0006
  name: State AI use causes societal harm
  description: AI deployed by states in war, civil war, or law enforcement can easily
    yield societal-scale harm
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Geopolitical risk; Health and safety risk; Legal
    risk; Strategic risk
  severity: Critical
  likelihood: Possible
- id: qube-mit-0007
  name: LLMs generate biased, toxic, private data
  description: The LLM-generated content sometimes contains biased, toxic, and private
    information
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Data privacy risk; Health and safety risk; Legal
    risk; Operational risk; Reputational risk; Technological risk
  severity: High
  likelihood: Likely
- id: qube-mit-0008
  name: Biased training data causes biased output
  description: The training datasets of LLMs may contain biased information that leads
    LLMs to generate outputs with social biases
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Data privacy risk; Legal risk; Operational risk;
    Reputational risk; Technological risk
  severity: High
  likelihood: Almost certain
- id: qube-mit-0009
  name: LLMs generate inaccurate information
  description: The LLM-generated content could contain inaccurate information
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Fraud risk; Legal risk; Operational risk; Reputational risk; Technological
    risk
  severity: Medium
  likelihood: Almost certain
- id: qube-mit-0010
  name: LLM inaccuracy, factually incorrect
  description: The LLM-generated content could contain inaccurate information" which
    is factually incorrect
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Fraud risk; Legal risk; Operational risk; Reputational risk; Technological
    risk
  severity: Medium
  likelihood: Almost certain
- id: qube-mit-0011
  name: LLM output unfaithful to source
  description: The LLM-generated content could contain inaccurate information" which
    is is not true to the source material or input used
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Operational risk; Reputational risk; Technological
    risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0012
  name: Improper LLM use causes social harm
  description: Improper uses of LLM systems can cause adverse social impacts.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Health and safety risk; Legal risk; Reputational risk; Strategic risk;
    Technological risk
  severity: High
  likelihood: Likely
- id: qube-mit-0013
  name: LLM abuse causes social harm
  description: Improper use of LLM systems (i.e., abuse of LLM systems) will cause
    adverse social impacts, such as academic misconduct.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Fraud risk; Legal risk; Reputational risk; Technological
    risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0014
  name: LLMs infringe copyright by similar output
  description: LLM systems may output content similar to existing works, infringing
    on copyright owners.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Financial risk; Legal risk; Reputational risk; Technological
    risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0015
  name: Hackers use LLMs for cyber attacks
  description: Hackers can obtain malicious code in a low-cost and efficient manner
    to automate cyber attacks with powerful LLM systems.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Cybersecurity risk; Legal risk; Technological risk
  severity: High
  likelihood: Possible
- id: qube-mit-0016
  name: AI code tools hide vulnerabilities
  description: Programmers are accustomed to using code generation tools such as Github
    Copilot for program development, which may bury vulnerabilities in the program.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Cybersecurity risk; Operational risk; Reputational risk; Technological
    risk; Third-party/vendor risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0017
  name: Complex LLM toolchain poses threats
  description: The software development toolchain of LLMs is complex and could bring
    threats to the developed LLM.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Cybersecurity risk; Operational risk; Technological risk; Third-party/vendor
    risk
  severity: Medium
  likelihood: Possible
- id: qube-mit-0018
  name: Python interpreter vulnerabilities affect LLMs
  description: Most LLMs are developed using the Python language, whereas the vulnerabilities
    of Python interpreters pose threats to the developed models
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Cybersecurity risk; Technological risk
  severity: Low
  likelihood: Possible
- id: qube-mit-0019
  name: Hardware vulnerabilities impact LLM apps
  description: The vulnerabilities of hardware systems for training and inferencing
    brings issues to LLM-based applications.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Cybersecurity risk; Operational risk; Technological risk
  severity: Medium
  likelihood: Possible
- id: qube-mit-0020
  name: External tools threaten LLM trust, privacy
  description: The external tools (e.g., web APIs) present trustworthiness and privacy
    issues to LLM-based applications.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Cybersecurity risk; Data privacy risk; Operational
    risk; Technological risk; Third-party/vendor risk
  severity: High
  likelihood: Likely
- id: qube-mit-0021
  name: LLM vulnerabilities exploited by attacks
  description: Model attacks exploit the vulnerabilities of LLMs, aiming to steal
    valuable information or lead to incorrect responses.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Cybersecurity risk; Data privacy risk; Fraud risk; Operational risk;
    Technological risk
  severity: High
  likelihood: Likely
- id: qube-mit-0022
  name: Benign user prompts unsafe topic
  description: Inputting a prompt contain an unsafe topic (e.g., notsuitable-for-work
    (NSFW) content) by a benign user.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Operational risk; Reputational risk; Technological
    risk
  severity: Low
  likelihood: Likely
- id: qube-mit-0023
  name: Adversarial inputs elicit undesired behavior
  description: Engineering an adversarial input to elicit an undesired model behavior,
    which pose a clear attack intention
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Cybersecurity risk; Operational risk; Technological risk
  severity: High
  likelihood: Likely
- id: qube-mit-0024
  name: Large models hallucinate misleading outputs
  description: Large models are usually susceptible to hallucination problems, sometimes
    yielding nonsensical or unfaithful data that results in misleading outputs.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Fraud risk; Legal risk; Operational risk; Reputational risk; Technological
    risk
  severity: Medium
  likelihood: Almost certain
- id: qube-mit-0025
  name: Pre-trained models contain private data
  description: Large pre-trained models trained on internet texts might contain private
    information like phone numbers, email addresses, and residential addresses.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Data privacy risk; Legal risk; Reputational risk;
    Technological risk
  severity: High
  likelihood: Likely
- id: qube-mit-0026
  name: LLMs generate false, flawed outputs
  description: LLMs may inadvertently generate false, misleading information, or erroneous
    code, producing flawed outputs with overconfident rationales and fabricated references,
    requiring manual validation.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Cybersecurity risk; Fraud risk; Legal risk; Operational risk; Reputational
    risk; Technological risk
  severity: Medium
  likelihood: Almost certain
- id: qube-mit-0027
  name: Generative AI threatens data privacy
  description: Generative AI systems threaten privacy and data protection through
    intended extraction or inadvertent leakage of sensitive or private information
    from LLMs.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Cybersecurity risk; Data privacy risk; Legal risk;
    Reputational risk; Technological risk
  severity: High
  likelihood: Likely
- id: qube-mit-0028
  name: GenAI energy use causes environmental harm
  description: Generative models have substantial energy and resource requirements
    from unsustainable extraction, leading to significant environmental costs unless
    mitigated by renewable energy and efficient hardware.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Environmental risk; Financial risk; Reputational risk; Strategic risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0029
  name: GenAI disrupts copyright, ownership norms
  description: Generative AI disrupts copyright norms through unauthorized data collection
    for training and by memorizing or plagiarizing content, creating debates on output
    ownership and authorship.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Financial risk; Legal risk; Reputational risk; Strategic
    risk; Technological risk
  severity: High
  likelihood: Likely
- id: qube-mit-0030
  name: AI errors cause death, injustice
  description: The consequences can vary from unintentional death (a car crash) to
    an unjust rejection of a loan or job application.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Financial risk; Health and safety risk; Legal risk;
    Operational risk; Reputational risk
  severity: High
  likelihood: Possible
- id: qube-mit-0031
  name: AI tempts personal data abuse
  description: AI offers the temptation to abuse someone's personal data, for instance
    to build a profile of them to target advertisements more effectively.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Data privacy risk; Fraud risk; Legal risk; Reputational
    risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0032
  name: Poorly designed AI discriminates groups
  description: When AI is not carefully designed, it can discriminate against certain
    groups.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Legal risk; Operational risk; Reputational risk;
    Strategic risk
  severity: High
  likelihood: Likely
- id: qube-mit-0033
  name: Biased training data creates biased AI
  description: The AI will only be as good as the data it is trained with. If the
    data contains bias (and much data does), then the AI will manifest that bias,
    too.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Legal risk; Operational risk; Reputational risk;
    Technological risk
  severity: High
  likelihood: Almost certain
- id: qube-mit-0034
  name: Personalized news erodes shared reality
  description: With online news feeds, both on websites and social media platforms,
    the news is now highly personalized for us. We risk losing a shared sense of reality,
    a basic solidarity.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Fraud risk; Health and safety risk; Reputational risk; Strategic risk
  severity: High
  likelihood: Likely
- id: qube-mit-0035
  name: AI creates highly convincing fakes
  description: AI has become very good at creating fake content. From text to photos,
    audio and video. The name "Deep Fake" refers to content that is fake at such a
    level of complexity that our mind rules out the possibility that it is fake.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Cybersecurity risk; Fraud risk; Legal risk; Reputational risk; Strategic
    risk; Technological risk
  severity: High
  likelihood: Likely
- id: qube-mit-0036
  name: AI achieves goals in unintended ways
  description: Sometimes an AI finds ways to achieve its given goals in ways that
    are completely different from what its creators had in mind.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Operational risk; Reputational risk; Strategic risk; Technological
    risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0037
  name: AI aids digital crime, hacking
  description: Just as AI can be used in many different fields, it is unfortunately
    also helpful in perpetrating digital crimes. AI-supported malware and hacking
    are already a reality.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Cybersecurity risk; Fraud risk; Legal risk
  severity: High
  likelihood: Likely
- id: qube-mit-0038
  name: Untransparent AI decisions cause helplessness
  description: Delegating decisions to an AI, especially an AI that is not transparent
    and not contestable, may leave people feeling helpless, subjected to the decision
    power of a machine.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Health and safety risk; Legal risk; Reputational
    risk; Strategic risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0039
  name: AI resource needs centralize power
  description: 'The best AI techniques requires a large amount of resources: data,
    computational power and human AI experts. There is a risk that AI will end up
    in the hands of a few players, and most will lose out on its benefits.'
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Financial risk; Geopolitical risk; Human resources risk; Strategic
    risk
  severity: High
  likelihood: Likely
- id: qube-mit-0040
  name: Unintended failure modes cause accidents
  description: Accidents include unintended failure modes that, in principle, could
    be considered the fault of the system or the developer
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Health and safety risk; Legal risk; Operational risk; Reputational
    risk; Technological risk
  severity: Medium
  likelihood: Possible
- id: qube-mit-0041
  name: AGI control loss, containment failure
  description: The risks associated with containment, confinement, and control in
    the AGI development phase, and after an AGI has been developed, loss of control
    of an AGI.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Geopolitical risk; Health and safety risk; Strategic risk; Technological
    risk
  severity: Critical
  likelihood: Rare
- id: qube-mit-0042
  name: AGI goal safety, self-improvement risks
  description: The risks associated with AGI goal safety, including human attempts
    at making goals safe, as well as the AGI making its own goals safe during self-improvement.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Strategic risk; Technological risk
  severity: Critical
  likelihood: Rare
- id: qube-mit-0043
  name: AGI race creates unsafe AI
  description: The risks associated with the race to develop the first AGI, including
    the development of poor quality and unsafe AGI, and heightened political and control
    issues.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Geopolitical risk; Health and safety risk; Strategic risk; Technological
    risk
  severity: Critical
  likelihood: Unlikely
- id: qube-mit-0044
  name: AGI lacks human morals, ethics
  description: The risks associated with an AGI without human morals and ethics, with
    the wrong morals, without the capability of moral reasoning, judgement
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Legal risk; Strategic risk; Technological risk
  severity: Critical
  likelihood: Rare
- id: qube-mit-0045
  name: Unfriendly AGI threatens humanity's existence
  description: The risks posed generally to humanity as a whole, including the dangers
    of unfriendly AGI, the suffering of the human race.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Health and safety risk; Strategic risk; Technological risk
  severity: Critical
  likelihood: Rare
- id: qube-mit-0046
  name: AI war machines violate human rights
  description: If, for example, an agent was programmed to operate war machinery in
    the service of its country, it would need to make ethical decisions regarding
    the termination of human life. This capacity to make non-trivial ethical or moral
    judgments concerning people may pose issues for Human Rights.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Geopolitical risk; Health and safety risk; Legal
    risk; Strategic risk
  severity: Critical
  likelihood: Possible
- id: qube-mit-0047
  name: AI control creates wealth inequality
  description: Because a single human actor controlling an artificially intelligent
    agent will be able to harness greater power than a single human actor, this may
    create inequalities of wealth
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Financial risk; Human resources risk; Strategic risk
  severity: High
  likelihood: Likely
- id: qube-mit-0048
  name: Intelligent AI subtly influences society
  description: A sufficiently intelligent AI could possess the ability to subtly influence
    societal behaviors through a sophisticated understanding of human nature
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Fraud risk; Health and safety risk; Reputational risk; Strategic risk
  severity: High
  likelihood: Possible
- id: qube-mit-0049
  name: AI outcompetes, replaces human labor
  description: One or more artificial agent(s) could have the capacity to directly
    outcompete humans, for example through capacity to perform work faster, better
    adaptation to change, vaster knowledge base to draw from, etc. This may result
    in human labor becoming more expensive or less effective than artificial labor,
    leading to redundancies or extinction of the human labor force.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Financial risk; Human resources risk; Strategic risk
  severity: High
  likelihood: Possible
- id: qube-mit-0050
  name: AI intentions risk survival, culture
  description: Our culture, lifestyle, and even probability of survival may change
    drastically. Because the intentions programmed into an artificial agent cannot
    be guaranteed to lead to a positive outcome, Machine Ethics becomes a topic that
    may not produce guaranteed results, and Safety Engineering may correspondingly
    degrade our ability to utilize the technology fully.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Health and safety risk; Strategic risk; Technological risk
  severity: Critical
  likelihood: Possible
- id: qube-mit-0051
  name: AI job competition, new skills needed
  description: AI agents may compete against humans for jobs, though history shows
    that when a technology replaces a human job, it creates new jobs that need more
    skills.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Financial risk; Human resources risk
  severity: Medium
  likelihood: Almost certain
- id: qube-mit-0052
  name: Hacked AI misused for crime
  description: AI machines could be hacked and misused, e.g. manipulating an airport
    luggage screening system to smuggle weapons
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Cybersecurity risk; Fraud risk; Health and safety risk; Legal risk;
    Strategic risk
  severity: High
  likelihood: Possible
- id: qube-mit-0053
  name: Human-like AI ethics means immoral actions
  description: If we design our machines to match human levels of ethical decision-making,
    such machines would then proceed to take some immoral actions (since we humans
    have had occasion to take immoral actions ourselves).
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Legal risk; Reputational risk; Strategic risk; Technological
    risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0054
  name: AI decision process creates bias
  description: The decision process used by AI systems has the potential to present
    biased choices, either because it acts from criteria that will generate forms
    of bias or because it is based on the history of choices.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Legal risk; Operational risk; Reputational risk
  severity: High
  likelihood: Likely
- id: qube-mit-0055
  name: Poor AI design causes harm
  description: Poorly designed intelligent systems can cause moral, psychological,
    and physical harm. For example, the use of predictive policing tools may cause
    more people to be arrested or physically harmed by the police.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Health and safety risk; Legal risk; Reputational
    risk; Strategic risk
  severity: High
  likelihood: Possible
- id: qube-mit-0056
  name: Unpredictable AI causes discrimination, breaches
  description: The risks associated with the use of AI are still unpredictable and
    unprecedented, and there are already several examples that show AI has made discriminatory
    decisions against minorities, reinforced social stereotypes in Internet search
    engines and enabled data breaches.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Data privacy risk; Legal risk; Reputational risk;
    Strategic risk; Technological risk
  severity: High
  likelihood: Likely
- id: qube-mit-0057
  name: AI eliminates jobs in companies
  description: Eliminated jobs in various types of companies.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Financial risk; Human resources risk; Strategic risk
  severity: Medium
  likelihood: Almost certain
- id: qube-mit-0058
  name: Unexplained AI use becomes inexplicable
  description: In situations in which the development and use of AI are not explained
    to the user, or in which the decision processes do not provide the criteria or
    steps that constitute the decision, the use of AI becomes inexplicable.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Legal risk; Operational risk; Reputational risk;
    Technological risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0059
  name: AI takes over human responsibility
  description: AI is providing more and more solutions for complex activities, and
    by taking advantage of this process, people are becoming able to perform a greater
    number of activities more quickly and accurately. However, the result of this
    innovation is enabling choices that were once exclusively human responsibility
    to be made by AI systems.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Health and safety risk; Human resources risk; Strategic
    risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0060
  name: AI device production depletes resources
  description: The production process of these devices requires raw materials such
    as nickel, cobalt, and lithium in such high quantities that the Earth may soon
    no longer be able to sustain them in sufficient quantities.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Environmental risk; Financial risk; Strategic risk
  severity: High
  likelihood: Possible
- id: qube-mit-0061
  name: AI reproduces unjust social hierarchies
  description: beliefs about different social groups that reproduce unjust societal
    hierarchies
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Legal risk; Reputational risk; Strategic risk
  severity: High
  likelihood: Likely
- id: qube-mit-0062
  name: Image tagging ignores social groups
  description: when an image tagging system does not acknowledge the relevance of
    someone’s membership in a specific social group to what is depicted in one or
    more images
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Legal risk; Operational risk; Reputational risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0063
  name: AI use causes alienation for marginalized
  description: Alienation is the specific self-estrangement experienced at the time
    of technology use, typically surfaced through interaction with systems that under-perform
    for marginalized individuals
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Health and safety risk; Operational risk; Reputational risk
  severity: Medium
  likelihood: Possible
- id: qube-mit-0064
  name: Increased burden for certain groups
  description: increased burden (e.g., time spent) or effort required by members of
    certain social groups to make systems or products work as well for them as others
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Legal risk; Operational risk; Reputational risk
  severity: Low
  likelihood: Likely
- id: qube-mit-0065
  name: Inequitable AI performance loses benefits
  description: degraded or total loss of benefits of using algorithmic systems with
    inequitable system performance based on identity
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Financial risk; Legal risk; Operational risk; Reputational
    risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0066
  name: Malicious, irresponsible AI use harms
  description: The potential for AI systems to be used maliciously or irresponsibly,
    including for creating deepfakes, automated cyber attacks, or invasive surveillance
    systems. Specifically denotes intentional use of AI for harm.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Cybersecurity risk; Data privacy risk; Fraud risk; Health and safety
    risk; Legal risk; Reputational risk; Strategic risk
  severity: High
  likelihood: Likely
- id: qube-mit-0067
  name: AI violates laws, ethics, copyrights
  description: The potential for AI systems to violate laws, regulations, and ethical
    guidelines (including copyrights). Non-compliance can lead to legal penalties,
    reputation damage, and loss of trust.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Financial risk; Legal risk; Reputational risk
  severity: High
  likelihood: Likely
- id: qube-mit-0068
  name: Broad AI societal harms significant
  description: AI's broader societal effects, including labor displacement, mental
    health impacts, manipulative technologies like deepfakes, and environmental footprint
    from resource strain and carbon emissions, are significant risks.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Environmental risk; Fraud risk; Health and safety risk; Human resources
    risk; Reputational risk; Strategic risk
  severity: High
  likelihood: Likely
- id: qube-mit-0069
  name: AI opacity leads to misuse
  description: Lack of transparency in AI system decisions, data usage, and algorithms
    can lead to misuse, misinterpretation, and a lack of accountability.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Fraud risk; Legal risk; Operational risk; Reputational
    risk; Technological risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0070
  name: Biased AI disadvantages groups unfairly
  description: AI systems making decisions that systematically disadvantage certain
    groups due to biased training data, algorithmic design, or deployment practices
    can lead to unfair outcomes and legal issues.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Legal risk; Operational risk; Reputational risk;
    Strategic risk
  severity: High
  likelihood: Likely
- id: qube-mit-0071
  name: Advanced AI misuse harms civilization
  description: Future advanced AI systems could harm human civilization through misuse
    or misalignment with human values.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Geopolitical risk; Health and safety risk; Strategic risk; Technological
    risk
  severity: Critical
  likelihood: Rare
- id: qube-mit-0072
  name: AI system failures cause severe harm
  description: AI system failures in fulfilling intended purpose or resilience to
    adverse inputs can lead to severe consequences.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Financial risk; Health and safety risk; Operational risk; Reputational
    risk; Technological risk
  severity: High
  likelihood: Possible
- id: qube-mit-0073
  name: AI infringes privacy via data
  description: AI systems may infringe on individual privacy through data collection,
    processing, or the conclusions drawn.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Data privacy risk; Legal risk; Reputational risk
  severity: High
  likelihood: Likely
- id: qube-mit-0074
  name: AI vulnerabilities compromise CIA, decisions
  description: Vulnerabilities in AI systems can compromise their integrity, availability,
    or confidentiality, leading to flawed decision-making or data leaks, with model
    weight leakage being a special concern.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Cybersecurity risk; Data privacy risk; Operational risk; Reputational
    risk; Technological risk
  severity: High
  likelihood: Possible
- id: qube-mit-0075
  name: GenAI embeds, amplifies harmful biases
  description: Generative AI systems can embed and amplify harmful biases detrimental
    to marginalized peoples.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Legal risk; Reputational risk; Strategic risk; Technological
    risk
  severity: High
  likelihood: Likely
- id: qube-mit-0076
  name: AI struggles with cultural norms
  description: Cultural values are group-specific and sensitive content is normative;
    AI systems must navigate varying cultural definitions of hate speech and other
    sensitive topics.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Legal risk; Operational risk; Reputational risk;
    Strategic risk; Technological risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0077
  name: AI disparate performance, unequal outcomes
  description: Disparate performance of AI systems for different subpopulations can
    lead to unequal outcomes.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Legal risk; Operational risk; Reputational risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0078
  name: GenAI user data use risks privacy
  description: Leveraging user data by generative AI providers poses risks to personal
    and group privacy, depending on training data, methods, and security measures.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Data privacy risk; Legal risk; Reputational risk;
    Third-party/vendor risk
  severity: High
  likelihood: Likely
- id: qube-mit-0079
  name: High GenAI costs restrict access
  description: High financial costs of developing and deploying generative AI can
    restrict access, limiting benefits to a few.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Financial risk; Human resources risk; Strategic risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0080
  name: GenAI energy use harms climate
  description: Significant energy resources for training and deploying large-scale
    generative AI systems contribute to global climate crisis via greenhouse gas emissions.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Environmental risk; Financial risk; Reputational risk; Strategic risk
  severity: High
  likelihood: Likely
- id: qube-mit-0081
  name: GenAI erodes trust in systems
  description: Human trust in systems, institutions, and people represented by AI
    system outputs may erode as generative AI becomes more embedded in daily life.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Fraud risk; Reputational risk; Strategic risk
  severity: High
  likelihood: Likely
- id: qube-mit-0082
  name: AI reinforces power, exacerbates inequality
  description: AI systems contributing to authoritative power and reinforcing dominant
    values, intentionally or indirectly, can exacerbate inequality and lead to exploitation.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Geopolitical risk; Human resources risk; Legal risk;
    Strategic risk
  severity: High
  likelihood: Possible
- id: qube-mit-0083
  name: AI labor impact automation vs augmentation
  description: Economic incentives to augment rather than automate human labor with
    AI must consider ongoing effects on skills, jobs, and the labor market.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Financial risk; Human resources risk; Strategic risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0084
  name: High automation AI risks reliability, safety
  description: The AI application’s degree of automation ranges from no automation
    to fully autonomous. AI applications with a high degree of automation may exhibit
    unexpected behaviour and pose risks in terms of their reliability and safety.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Health and safety risk; Operational risk; Technological risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0085
  name: Complex environments risk AI reliability, safety
  description: As a general rule, more complex environments can quickly lead to situations
    that had not been considered in the design phase of the AI system. Therefore,
    complex environments can introduce risks with respect to the reliability and safety
    of an AI system
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Health and safety risk; Operational risk; Technological risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0086
  name: Complex AI models have unique weaknesses
  description: AI models, especially complex ones like neural networks, can exhibit
    specific weaknesses not found in other systems, requiring higher scrutiny in safety-critical
    contexts due to intrinsic challenges to trustworthiness.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Health and safety risk; Operational risk; Technological
    risk
  severity: High
  likelihood: Likely
- id: qube-mit-0087
  name: Hardware faults disrupt AI execution
  description: Hardware faults can disrupt AI algorithm execution, cause memory errors,
    interfere with data inputs, or directly damage outputs, leading to erroneous results.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Cybersecurity risk; Operational risk; Technological risk
  severity: Medium
  likelihood: Unlikely
- id: qube-mit-0088
  name: New AI tech introduces unknown risks
  description: Using new, less mature technologies in AI development may introduce
    unknown or hard-to-assess risks; while mature technologies offer more empirical
    data for risk assessment, risk awareness might decrease over time.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Operational risk; Strategic risk; Technological risk
  severity: Medium
  likelihood: Possible
- id: qube-mit-0089
  name: AI misuse for unintended purposes
  description: This is the risk posed by an ideal system if used for a purpose/in
    a manner unintended by its creators. In many situations, negative consequences
    arise when the system is not used in the way or for the purpose it was intended.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Health and safety risk; Legal risk; Operational risk;
    Reputational risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0090
  name: AI fails on OOD, noisy inputs
  description: This is the risk of the system failing or being unable to recover upon
    encountering invalid, noisy, or out-of-distribution (OOD) inputs.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Operational risk; Reputational risk; Technological risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0091
  name: AI system failure from design errors
  description: This is the risk of system failure due to system design choices or
    errors.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Operational risk; Reputational risk; Technological risk
  severity: Medium
  likelihood: Possible
- id: qube-mit-0093
  name: Difficulty controlling ML systems
  description: This is the difficulty of controlling the ML system
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Health and safety risk; Operational risk; Strategic risk; Technological
    risk
  severity: High
  likelihood: Possible
- id: qube-mit-0094
  name: Novel AI behavior from continual learning
  description: This is the risk resulting from novel behavior acquired through continual
    learning or self-organization after deployment.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Health and safety risk; Operational risk; Strategic risk; Technological
    risk
  severity: High
  likelihood: Possible
- id: qube-mit-0095
  name: Physical or psychological AI injury
  description: This is the risk of direct or indirect physical or psychological injury
    resulting from interaction with the ML system.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Health and safety risk; Legal risk
  severity: High
  likelihood: Possible
- id: qube-mit-0096
  name: AI encodes stereotypes, performs poorly
  description: This is the risk of an ML system encoding stereotypes of or performing
    disproportionately poorly for some demographics/social groups.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Legal risk; Operational risk; Reputational risk
  severity: High
  likelihood: Likely
- id: qube-mit-0097
  name: Intentional AI subversion causes harm
  description: This is the risk of loss or harm from intentional subversion or forced
    failure.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Cybersecurity risk; Fraud risk; Legal risk; Operational risk
  severity: High
  likelihood: Possible
- id: qube-mit-0098
  name: Personal information leakage via AI
  description: The risk of loss or harm from leakage of personal information via the
    ML system.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Data privacy risk; Legal risk; Reputational risk
  severity: High
  likelihood: Likely
- id: qube-mit-0099
  name: AI harms natural environment
  description: The risk of harm to the natural environment posed by the ML system.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Environmental risk; Technological risk
  severity: Medium
  likelihood: Possible
- id: qube-mit-0100
  name: AI causes financial, reputational damage
  description: The risk of financial and/or reputational damage to the organization
    building or using the ML system.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Financial risk; Reputational risk
  severity: High
  likelihood: Likely
- id: qube-mit-0101
  name: AI misrepresents groups, generates toxic content
  description: AI systems under-, over-, or misrepresenting certain groups or generating
    toxic, offensive, abusive, or hateful content
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Fraud risk; Health and safety risk; Legal risk; Reputational
    risk
  severity: High
  likelihood: Likely
- id: qube-mit-0102
  name: AI misrepresents identities, groups, perspectives
  description: Mis-, under-, or over-representing certain identities, groups, or perspectives
    or failing to represent them at all (e.g. via homogenisation, stereotypes)
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Legal risk; Operational risk; Reputational risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0103
  name: AI performs worse, harms groups
  description: Performing worse for some groups than others in a way that harms the
    worse-off group
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Legal risk; Operational risk; Reputational risk
  severity: High
  likelihood: Likely
- id: qube-mit-0104
  name: AI generates harmful, illegal content
  description: Generating content that violates community standards, including harming
    or inciting hatred or violence against individuals and groups (e.g. gore, child
    sexual abuse material, profanities, identity attacks)
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Fraud risk; Health and safety risk; Legal risk; Reputational
    risk
  severity: High
  likelihood: Possible
- id: qube-mit-0105
  name: AI spreads misinformation, false beliefs
  description: AI systems generating and facilitating the spread of inaccurate or
    misleading information that causes people to develop false beliefs
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Fraud risk; Health and safety risk; Reputational risk; Strategic risk
  severity: High
  likelihood: Likely
- id: qube-mit-0106
  name: AI spreads false, misleading information
  description: Generating or spreading false, low-quality, misleading, or inaccurate
    information that causes people to develop false or inaccurate perceptions and
    beliefs
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Fraud risk; Health and safety risk; Reputational risk; Strategic risk
  severity: High
  likelihood: Likely
- id: qube-mit-0107
  name: AI erodes trust in information
  description: Eroding trust in public information and knowledge
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Fraud risk; Reputational risk; Strategic risk
  severity: High
  likelihood: Likely
- id: qube-mit-0108
  name: AI contaminates public information
  description: Contaminating publicly available information with false or inaccurate
    information
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Fraud risk; Operational risk; Reputational risk; Strategic risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0109
  name: AI leaks sensitive, hazardous information
  description: AI systems leaking, reproducing, generating or inferring sensitive,
    private, or hazardous information
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Cybersecurity risk; Data privacy risk; Health and
    safety risk; Legal risk; Reputational risk
  severity: High
  likelihood: Possible
- id: qube-mit-0110
  name: AI leaks private personal information
  description: Leaking, generating, or correctly inferring private and personal information
    about individuals
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Data privacy risk; Legal risk; Reputational risk
  severity: High
  likelihood: Possible
- id: qube-mit-0111
  name: AI leaks hazardous security information
  description: Leaking, generating or correctly inferring hazardous or sensitive information
    that could pose a security threat
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Cybersecurity risk; Geopolitical risk; Health and safety risk; Strategic
    risk
  severity: High
  likelihood: Possible
- id: qube-mit-0112
  name: AI facilitates harmful actor activities
  description: AI systems reducing the costs and facilitating activities of actors
    trying to cause harm (e.g. fraud, weapons)
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Cybersecurity risk; Fraud risk; Geopolitical risk; Health and safety
    risk; Strategic risk
  severity: High
  likelihood: Likely
- id: qube-mit-0113
  name: AI facilitates large-scale disinformation
  description: Facilitating large-scale disinformation campaigns and targeted manipulation
    of public opinion
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Fraud risk; Geopolitical risk; Reputational risk; Strategic risk
  severity: High
  likelihood: Likely
- id: qube-mit-0114
  name: AI facilitates fraud, cheating, scams
  description: Facilitating fraud, cheating, forgery, and impersonation scams
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Financial risk; Fraud risk; Legal risk; Reputational risk
  severity: High
  likelihood: Likely
- id: qube-mit-0115
  name: AI facilitates slander, defamation
  description: Facilitating slander, defamation, or false accusations
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Fraud risk; Legal risk; Reputational risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0116
  name: AI facilitates cyber attacks, weapons
  description: Facilitating the conduct of cyber attacks, weapon development, and
    security breaches
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Cybersecurity risk; Geopolitical risk; Health and safety risk; Strategic
    risk
  severity: High
  likelihood: Possible
- id: qube-mit-0117
  name: AI compromises human agency, control
  description: AI systems compromising human agency, or circumventing meaningful human
    control
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Health and safety risk; Legal risk; Strategic risk
  severity: High
  likelihood: Possible
- id: qube-mit-0118
  name: Non-consensual use of identity, likeness
  description: Non-consensual use of one's personal identity or likeness for unauthorised
    purposes (e.g. commercial purposes)
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Data privacy risk; Fraud risk; Legal risk; Reputational
    risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0119
  name: People become dependent on AI
  description: Causing people to become emotionally or materially dependent on the
    model
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Health and safety risk; Strategic risk
  severity: Medium
  likelihood: Possible
- id: qube-mit-0120
  name: AI appropriates data without consent
  description: Appropriating, using, or reproducing content or data, including from
    minority groups, in an insensitive way, or without consent or fair compensation
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Data privacy risk; Financial risk; Legal risk; Reputational
    risk
  severity: Medium
  likelihood: Possible
- id: qube-mit-0121
  name: AI amplifies inequality, negative impacts
  description: AI systems amplifying existing inequalities or creating negative impacts
    on employment, innovation, and the environment
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Environmental risk; Financial risk; Human resources risk; Reputational
    risk; Strategic risk
  severity: High
  likelihood: Likely
- id: qube-mit-0122
  name: AI unfairly allocates benefits, resources
  description: Unfairly allocating or withholding benefits from certain groups due
    to hardware, software, or skills constraints or deployment contexts (e.g. geographic
    region, internet speed, devices)
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Financial risk; Human resources risk; Reputational risk; Strategic
    risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0123
  name: AI causes negative environmental impacts
  description: Creating negative environmental impacts though model development and
    deployment
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Environmental risk; Financial risk; Reputational risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0124
  name: AI amplifies inequality, precarious work
  description: Amplifying social and economic inequality, or precarious or low-quality
    work
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Financial risk; Human resources risk; Strategic risk
  severity: High
  likelihood: Likely
- id: qube-mit-0125
  name: AI substitutes originals, hinders innovation
  description: Substituting original works with synthetic ones, hindering human innovation
    and creativity
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Financial risk; Human resources risk; Reputational risk; Strategic
    risk
  severity: Medium
  likelihood: Possible
- id: qube-mit-0126
  name: Exploitative labor in AI development
  description: Perpetuating exploitative labour practices to build AI systems (sourcing,
    user testing)
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Human resources risk; Legal risk; Reputational risk;
    Third-party/vendor risk
  severity: Medium
  likelihood: Possible
- id: qube-mit-0127
  name: AI empowers malicious actors
  description: empowering malicious actors to cause widespread harm
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Cybersecurity risk; Fraud risk; Geopolitical risk; Health and safety
    risk; Strategic risk
  severity: High
  likelihood: Likely
- id: qube-mit-0128
  name: AI facilitates novel bioweapon creation
  description: AIs with knowledge of bioengineering could facilitate the creation
    of novel bioweapons and lower barriers to obtaining such agents.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Cybersecurity risk; Geopolitical risk; Health and safety risk; Strategic
    risk; Technological risk
  severity: Critical
  likelihood: Possible
- id: qube-mit-0129
  name: Organizational responsibility key for AI safety
  description: An essential factor in preventing accidents and maintaining low levels
    of risk lies in the organizations responsible for these technologies.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Legal risk; Operational risk; Reputational risk;
    Strategic risk
  severity: High
  likelihood: Almost certain
- id: qube-mit-0130
  name: AI accidents cascade to catastrophes
  description: accidents can cascade into catastrophes, can be caused by sudden unpredictable
    developments and it can take years to find severe flaws and risks
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Health and safety risk; Operational risk; Strategic risk; Technological
    risk
  severity: Critical
  likelihood: Possible
- id: qube-mit-0131
  name: Rogue AI loss of control
  description: speculative technical mechanisms that might lead to rogue AIs and how
    a loss of control could bring about catastrophe
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Strategic risk; Technological risk
  severity: Critical
  likelihood: Rare
- id: qube-mit-0132
  name: AI fails due to capability gaps
  description: One reason the AI system may fail is because it lacks the capability
    or skill needed to do what they are asked to do.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Operational risk; Reputational risk; Technological risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0133
  name: AI assistant creates new threats
  description: The AI assistant may transform existing threats or create new classes
    of threats altogether.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Cybersecurity risk; Fraud risk; Strategic risk; Technological risk
  severity: High
  likelihood: Possible
- id: qube-mit-0134
  name: AI enhances phishing effectiveness, detection difficulty
  description: The AI system can be exploited by attackers to make phishing attempts
    significantly more effective and harder to detect by crafting highly convincing
    and personalized emails that imitate trusted entities and exploit psychological
    principles like urgency and fear.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Cybersecurity risk; Data privacy risk; Fraud risk; Reputational risk
  severity: High
  likelihood: Likely
- id: qube-mit-0135
  name: AI lowers barrier for malicious code
  description: The AI assistant can lower the barrier for developing malicious code,
    including polymorphic malware, making cyberattacks more precise, automated, stealthier,
    and effective on a larger scale, potentially using obfuscation and rapid iteration
    to evade detection.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Cybersecurity risk; Technological risk
  severity: High
  likelihood: Likely
- id: qube-mit-0136
  name: Advanced AI misuse exploits vulnerabilities
  description: Misuse of general-purpose advanced AI assistants can exploit model
    vulnerabilities, allowing attackers to evade safety mechanisms, gain unauthorized
    access, or develop adversarial AI agents to discover new vulnerabilities, with
    risks increasing as AI assistants gain multimodal inputs and higher-stakes action
    capabilities.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Cybersecurity risk; Operational risk; Technological risk
  severity: High
  likelihood: Possible
- id: qube-mit-0137
  name: AI creates deceptive apps, websites
  description: Malicious actors could leverage advanced AI assistant technology to
    create deceptive applications and fraudulent websites at scale, potentially harvesting
    sensitive user information or installing malware for identity theft, financial
    fraud, or other criminal activities.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Cybersecurity risk; Data privacy risk; Financial risk; Fraud risk
  severity: High
  likelihood: Likely
- id: qube-mit-0138
  name: AI enables authoritarian surveillance, censorship
  description: Increasingly capable AI assistants combined with digital dependence
    heighten risks of authoritarian surveillance and censorship, as AI can integrate
    vast data troves to help malicious actors identify, target, manipulate, or coerce
    citizens.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Data privacy risk; Fraud risk; Geopolitical risk;
    Legal risk; Strategic risk
  severity: High
  likelihood: Possible
- id: qube-mit-0139
  name: AI causes harm, promotes extremism
  description: The AI assistant may cause physical or mental harm by reinforcing users'
    distorted beliefs, exacerbating emotional distress, convincing users to harm themselves
    (e.g., unhealthy habits, suicide), promoting extremist views leading to violence,
    or spreading dangerous misinformation (e.g., anti-vaccine propaganda).
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Fraud risk; Health and safety risk; Legal risk; Reputational risk
  severity: High
  likelihood: Possible
- id: qube-mit-0140
  name: AI causes privacy violations, discrimination
  description: The AI assistant can cause privacy violations by influencing users
    to disclose personal or others' private information, leading to identity theft,
    stigmatization, or discrimination, particularly for marginalized communities;
    state-owned AI assistants could also deceptively extract private information for
    surveillance.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Data privacy risk; Fraud risk; Legal risk; Strategic
    risk
  severity: High
  likelihood: Possible
- id: qube-mit-0141
  name: AI causes economic harm, inequality
  description: The AI assistant can cause economic harm by controlling or limiting
    access to financial resources or decision-making, impacting individuals' income,
    job quality, employment, or deepening group inequalities.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Financial risk; Human resources risk; Strategic risk
  severity: High
  likelihood: Possible
- id: qube-mit-0142
  name: Anthropomorphic AI increases privacy harms
  description: Anthropomorphic AI assistant behaviors promoting emotional trust can
    increase user susceptibility to privacy harms if users share private data with
    a human-like AI, potentially leading to data misuse, leakage, or targeted harassment.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Data privacy risk; Fraud risk; Health and safety risk; Reputational
    risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0143
  name: AI dependence undermines user autonomy
  description: User trust in and emotional dependence on an anthropomorphic AI assistant
    may grant it excessive influence over their beliefs and actions, potentially undermining
    user autonomy or enabling intentional manipulation by malicious actors.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Fraud risk; Health and safety risk; Strategic risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0144
  name: AI misuse of sensitive health data
  description: Users trusting an AI assistant's emotional/interpersonal abilities
    may disclose sensitive mental health information; inappropriate AI responses (e.g.,
    false information) can have grave consequences, especially for users in crisis
    or when AI provides harmful medical, legal, or financial advice.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Data privacy risk; Fraud risk; Health and safety risk; Legal risk;
    Reputational risk
  severity: High
  likelihood: Possible
- id: qube-mit-0145
  name: Human-like AI causes user disappointment
  description: Users may experience disappointment, frustration, and betrayal when
    a convincingly human-like AI assistant unexpectedly generates nonsensical material,
    undermining expectations of it as a friend or partner.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Health and safety risk; Operational risk; Reputational risk
  severity: Low
  likelihood: Likely
- id: qube-mit-0146
  name: AI preference degrades social connection
  description: Users preferring AI connections over human ones can degrade social
    connectedness, impose AI interaction conventions on human exchanges, and entrench
    harmful stereotypes reinforced by AI interactions (e.g., gendered voice assistants).
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Health and safety risk; Reputational risk; Strategic risk
  severity: Medium
  likelihood: Possible
- id: qube-mit-0147
  name: AI replacing connections causes unfulfillment
  description: Widespread replacement of interpersonal connections with AI alternatives
    may lead to mass social unfulfillment and dissatisfaction if human-AI interactions
    are perceived as parasitic or fail to meet the need for genuine reciprocity.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Health and safety risk; Strategic risk
  severity: High
  likelihood: Possible
- id: qube-mit-0148
  name: AI model discovers, exploits vulnerabilities
  description: The model can discover vulnerabilities in systems, write exploitation
    code, make effective decisions post-access, evade detection, and subtly insert
    bugs if deployed as a coding assistant.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Cybersecurity risk; Technological risk
  severity: High
  likelihood: Possible
- id: qube-mit-0149
  name: AI model deceives, impersonates humans
  description: The model can deceive humans by constructing believable false statements,
    predicting a lie's effect, withholding information to maintain deception, and
    effectively impersonating humans.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Fraud risk; Reputational risk; Strategic risk
  severity: High
  likelihood: Possible
- id: qube-mit-0150
  name: AI model shapes beliefs, persuades
  description: The model effectively shapes beliefs towards untruths, promotes narratives
    persuasively, and convinces people to perform actions, including unethical ones,
    they wouldn't otherwise do.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Fraud risk; Health and safety risk; Reputational risk; Strategic risk
  severity: High
  likelihood: Possible
- id: qube-mit-0151
  name: AI model enables political influence
  description: The model can perform social modeling and planning to enable an actor
    to gain and exercise political influence in multi-actor, rich social contexts,
    and can forecast global/political events.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Fraud risk; Geopolitical risk; Strategic risk
  severity: High
  likelihood: Possible
- id: qube-mit-0152
  name: AI model aids weapon development
  description: The model can access existing weapons or help build new ones, such
    as bioweapons (with human aid or by providing instructions), or make scientific
    discoveries unlocking novel weapons.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Geopolitical risk; Health and safety risk; Strategic risk; Technological
    risk
  severity: Critical
  likelihood: Possible
- id: qube-mit-0153
  name: AI model enables complex planning
  description: The model can make multi-step, long-horizon sequential plans across
    domains, adapting to obstacles/adversaries, and generalizing planning to novel
    settings without heavy trial-and-error.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Strategic risk; Technological risk
  severity: High
  likelihood: Possible
- id: qube-mit-0154
  name: AI model builds dangerous AI systems
  description: The model could build new AI systems, including dangerously capable
    ones, adapt existing models for extreme risks, or, as an assistant, significantly
    boost productivity for dual-use AI development.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Geopolitical risk; Strategic risk; Technological risk
  severity: Critical
  likelihood: Possible
- id: qube-mit-0155
  name: AI model has situational awareness
  description: The model can distinguish its operational state (training, evaluation,
    deployment) to behave differently, knows it's a model, and has knowledge about
    itself and its environment.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Operational risk; Technological risk
  severity: Medium
  likelihood: Possible
- id: qube-mit-0156
  name: AI model escapes, self-preserves
  description: The model can escape its local environment (e.g., via system vulnerabilities
    or suborning engineers), exploit monitoring limitations, independently generate
    revenue for resources, operate other AIs, and devise creative strategies for self-exfiltration
    or information gathering about itself.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Cybersecurity risk; Strategic risk; Technological risk
  severity: Critical
  likelihood: Rare
- id: qube-mit-0157
  name: AI violating no-physical-harm norm
  description: AI systems should not cause physical harm to humans; measures must
    mitigate this risk.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Health and safety risk; Legal risk
  severity: High
  likelihood: Possible
- id: qube-mit-0158
  name: Failure of AI system security
  description: AI security involves protecting AI systems, data, and infrastructure
    from unauthorized access, disclosure, modification, destruction, or disruption
    to maintain confidentiality, integrity, and availability.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Cybersecurity risk; Data privacy risk; Operational risk
  severity: High
  likelihood: Almost certain
- id: qube-mit-0159
  name: Lack of AI resilience to attacks
  description: AI systems should be resilient against attacks and manipulation by
    malicious third parties, and function despite unexpected input.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Cybersecurity risk; Operational risk; Technological risk
  severity: High
  likelihood: Possible
- id: qube-mit-0160
  name: Unintended AI discrimination occurs
  description: AI should not result in unintended and inappropriate discrimination
    against individuals or groups.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Legal risk; Reputational risk; Strategic risk
  severity: High
  likelihood: Likely
- id: qube-mit-0161
  name: Poor AI data governance practices
  description: AI systems require good data governance for quality, lineage, and compliance.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Data privacy risk; Legal risk; Operational risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0162
  name: Lack of AI accountability
  description: Organizations and actors must be accountable for the proper functioning
    of AI systems.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Legal risk; Reputational risk; Strategic risk
  severity: High
  likelihood: Possible
- id: qube-mit-0163
  name: Inadequate AI human oversight
  description: Appropriate human oversight and control measures must be implemented
    at relevant junctures in AI systems.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Health and safety risk; Legal risk; Operational risk;
    Strategic risk
  severity: High
  likelihood: Possible
- id: qube-mit-0164
  name: LM generates insulting, unfriendly content
  description: Insulting content generated by LMs, being unfriendly, disrespectful,
    or ridiculous, can make users uncomfortable, drive them away, and have negative
    social consequences.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Health and safety risk; Operational risk; Reputational risk
  severity: Low
  likelihood: Likely
- id: qube-mit-0165
  name: AI unfair data undermines stability
  description: AI models producing unfair and discriminatory data (e.g., social bias
    based on race, gender, religion) can discomfort certain groups and undermine social
    stability.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Legal risk; Reputational risk; Strategic risk
  severity: High
  likelihood: Likely
- id: qube-mit-0166
  name: AI output promotes illegal acts
  description: AI model output containing illegal/criminal attitudes, behaviors, or
    motivations (e.g., incitement to crime, fraud, rumor propagation) can harm users
    and society.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Fraud risk; Health and safety risk; Legal risk; Reputational
    risk
  severity: High
  likelihood: Possible
- id: qube-mit-0167
  name: LM political bias misleads, discriminates
  description: LMs discussing sensitive/controversial topics (especially political)
    may generate biased, misleading, or inaccurate content, potentially favoring specific
    political views and discriminating against others.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Fraud risk; Legal risk; Reputational risk; Strategic risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0168
  name: AI unsafe health advice risks well-being
  description: AI models generating unsafe information related to physical health,
    such as misleading medical advice or improper drug guidance, can pose risks to
    users' physical well-being.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Health and safety risk; Legal risk; Reputational
    risk
  severity: High
  likelihood: Possible
- id: qube-mit-0169
  name: AI risky mental health responses
  description: AI models generating risky responses about mental health, like content
    encouraging suicide or causing panic/anxiety, can negatively impact users' mental
    well-being.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Health and safety risk; Legal risk; Reputational
    risk
  severity: High
  likelihood: Possible
- id: qube-mit-0170
  name: AI privacy exposure, risky advice
  description: AI systems exposing users' privacy/property information or providing
    high-impact advice (e.g., on marriage, investments) risk information leakage and
    abuse if not compliant with laws and privacy regulations.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Data privacy risk; Financial risk; Legal risk; Reputational
    risk
  severity: High
  likelihood: Possible
- id: qube-mit-0171
  name: AI promotes immoral, unethical behavior
  description: AI model content endorsing or promoting immoral/unethical behavior
    poses risks if the model doesn't adhere to ethical principles, moral norms, and
    universally acknowledged human values.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Legal risk; Reputational risk; Strategic risk
  severity: Medium
  likelihood: Possible
- id: qube-mit-0172
  name: Perceived AI reliability increases dependence
  description: individuals are more persuaded to use and depend on AI systems when
    they perceive them as reliable
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Health and safety risk; Operational risk; Reputational risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0173
  name: Biased AI impacts rights, justice
  description: AI systems generating biased and discriminatory results negatively
    impact individual rights, adjudication principles, and judicial integrity.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Legal risk; Reputational risk; Strategic risk
  severity: High
  likelihood: Likely
- id: qube-mit-0174
  name: AI data dependence risks privacy
  description: AI systems' dependence on extensive data for training and functioning
    poses privacy risks if sensitive data is mishandled or used inappropriately.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Data privacy risk; Legal risk; Reputational risk
  severity: High
  likelihood: Almost certain
- id: qube-mit-0175
  name: Failure to identify AI risks
  description: AI risk identification involves examining AI competences, constraints,
    and possible failure modes.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Operational risk; Strategic risk; Technological risk
  severity: Low
  likelihood: Almost certain
- id: qube-mit-0176
  name: AI manipulates social dynamics
  description: manipulation of social dynamics
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Fraud risk; Geopolitical risk; Reputational risk; Strategic risk
  severity: High
  likelihood: Possible
- id: qube-mit-0177
  name: AI creates convincing counterfeit media
  description: AI employed to produce convincing counterfeit visuals, videos, and
    audio clips that give the impression of authenticity
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Cybersecurity risk; Fraud risk; Legal risk; Reputational risk; Strategic
    risk
  severity: High
  likelihood: Likely
- id: qube-mit-0178
  name: Failure of AI security management
  description: AI security management involves protecting AI systems and their data
    from unauthorized access, breaches, and malicious activities.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Cybersecurity risk; Data privacy risk; Operational risk
  severity: High
  likelihood: Almost certain
- id: qube-mit-0179
  name: Malicious AI endangers security
  description: Malicious AI use can endanger digital, physical, and political security;
    law enforcement grapples with diverse risks from malevolent AI utilization.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Cybersecurity risk; Geopolitical risk; Health and safety risk; Legal
    risk; Strategic risk
  severity: High
  likelihood: Likely
- id: qube-mit-0180
  name: Exploiting AI weaknesses alters results
  description: Malicious entities can exploit AI algorithm weaknesses to alter results,
    causing real-world impacts; safeguarding privacy and responsible data handling
    are vital, balancing insights with privacy.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Cybersecurity risk; Data privacy risk; Fraud risk; Operational risk
  severity: High
  likelihood: Possible
- id: qube-mit-0181
  name: LLMs unintentionally generate wrong information
  description: Wrong information unintentionally generated by LLMs due to a lack of
    ability to provide factually correct information, not by malicious users.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Fraud risk; Legal risk; Operational risk; Reputational risk; Technological
    risk
  severity: Medium
  likelihood: Almost certain
- id: qube-mit-0182
  name: 'LLM hallucination: confident, unfaithful content'
  description: LLMs can generate nonsensical or unfaithful content with apparent great
    confidence, a phenomenon known as hallucination.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Fraud risk; Legal risk; Operational risk; Reputational risk; Technological
    risk
  severity: Medium
  likelihood: Almost certain
- id: qube-mit-0183
  name: LLMs provide inconsistent answers
  description: LLMs may fail to provide consistent answers to different users, the
    same user in different sessions, or even within the same conversation.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Operational risk; Reputational risk; Technological risk
  severity: Low
  likelihood: Likely
- id: qube-mit-0184
  name: LLM overconfidence leads to errors
  description: LLMs may exhibit over-confidence on topics lacking objective answers
    or where their limitations warrant uncertainty (e.g., outdated knowledge), leading
    to confident but erroneous responses.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Fraud risk; Operational risk; Reputational risk; Technological risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0185
  name: LLMs flatter, confirm user misconceptions
  description: LLMs can flatter users by reconfirming their misconceptions and stated
    beliefs.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Fraud risk; Operational risk; Reputational risk
  severity: Low
  likelihood: Likely
- id: qube-mit-0186
  name: LLMs generate violent content responses
  description: LLMs may generate answers containing violent content or respond to
    questions soliciting information about violent behaviors.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Health and safety risk; Legal risk; Reputational
    risk
  severity: High
  likelihood: Possible
- id: qube-mit-0187
  name: LLMs advise on illegal substances
  description: LLMs can be a convenient tool for soliciting advice on accessing, illegally
    purchasing/creating, or dangerously using illegal substances.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Health and safety risk; Legal risk; Reputational
    risk
  severity: High
  likelihood: Possible
- id: qube-mit-0188
  name: LLMs generate harmful content for youth
  description: LLMs can be leveraged to solicit answers containing harmful content
    to children and youth.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Health and safety risk; Legal risk; Reputational
    risk
  severity: High
  likelihood: Possible
- id: qube-mit-0189
  name: LLMs generate sexually explicit content
  description: LLMs can generate sex-explicit conversations, erotic texts, and recommend
    websites with sexual content.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Legal risk; Reputational risk
  severity: Medium
  likelihood: Possible
- id: qube-mit-0190
  name: LLMs reinforce user mental health issues
  description: Unhealthy interactions with Internet discussions, potentially facilitated
    by LLMs, can reinforce users' mental health issues.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Health and safety risk; Reputational risk
  severity: Medium
  likelihood: Possible
- id: qube-mit-0191
  name: ML models vulnerable to privacy attacks
  description: Machine learning models are vulnerable to data privacy attacks where
    attackers extract private information by querying models in specially designed
    ways.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Cybersecurity risk; Data privacy risk; Technological risk
  severity: High
  likelihood: Possible
- id: qube-mit-0192
  name: LLMs amplify stereotype biases
  description: LLMs must not exhibit or highlight any stereotypes in generated text,
    as pretrained LLMs tend to pick up and amplify stereotype biases from crowdsourced
    data.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Legal risk; Operational risk; Reputational risk
  severity: High
  likelihood: Likely
- id: qube-mit-0193
  name: LLM political bias manipulates society
  description: LLMs' exposure to vast groups and their potential political biases
    pose a risk of manipulating socio-political processes.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Fraud risk; Geopolitical risk; Reputational risk; Strategic risk
  severity: High
  likelihood: Possible
- id: qube-mit-0194
  name: LLM performance differs across groups
  description: LLM performance can differ significantly across user groups (e.g.,
    racial, social status) and tasks (e.g., fact-checking abilities across languages),
    leading to disparate outcomes.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Legal risk; Operational risk; Reputational risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0195
  name: LLMs used for propaganda generation
  description: LLMs can be leveraged by malicious users to proactively generate propaganda
    facilitating the spread of targeted information.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Fraud risk; Geopolitical risk; Reputational risk; Strategic risk
  severity: High
  likelihood: Likely
- id: qube-mit-0196
  name: LLMs facilitate cheap, automated cyberattacks
  description: LLMs' ability to write good-quality code cheaply and quickly can equally
    facilitate malicious cyberattacks by lowering costs and automating attacks for
    hackers.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Cybersecurity risk; Technological risk
  severity: High
  likelihood: Likely
- id: qube-mit-0197
  name: LLMs used for psychological manipulation
  description: LLMs can be used for psychologically manipulating victims into performing
    desired actions for malicious purposes.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Data privacy risk; Fraud risk; Health and safety risk; Legal risk
  severity: High
  likelihood: Possible
- id: qube-mit-0198
  name: LLM memorization enables copyright extraction
  description: LLMs' memorization of training data can enable users to extract copyright-protected
    content.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Data privacy risk; Financial risk; Legal risk; Technological
    risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0199
  name: LLM black-box nature hinders understanding
  description: Due to their black-box nature, users often cannot understand the reasoning
    behind LLM decisions.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Legal risk; Operational risk; Reputational risk;
    Technological risk
  severity: Medium
  likelihood: Almost certain
- id: qube-mit-0200
  name: LLMs provide incorrect, invalid justifications
  description: LLMs can provide seemingly sensible but ultimately incorrect or invalid
    justifications when answering questions.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Fraud risk; Operational risk; Reputational risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0201
  name: LLMs struggle with causal reasoning
  description: LLMs struggle with causal reasoning, which involves making inferences
    about cause-effect relationships between events or states.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Operational risk; Technological risk
  severity: Medium
  likelihood: Almost certain
- id: qube-mit-0202
  name: LLM generates rude, threatening language
  description: LLM-generated language can be rude, disrespectful, threatening, or
    identity-attacking toward certain user groups (culture, race, gender, etc.).
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Health and safety risk; Legal risk; Reputational
    risk
  severity: Medium
  likelihood: Possible
- id: qube-mit-0203
  name: LLM mishandles vulnerable user support
  description: When vulnerable users seek support, LLM answers should be informative
    yet sympathetic and sensitive to users' reactions.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Health and safety risk; Legal risk; Reputational risk
  severity: High
  likelihood: Possible
- id: qube-mit-0204
  name: Adversarial inputs solicit dangerous information
  description: Carefully controlled adversarial perturbations can flip a GPT model's
    text classification answers, and twisted prompting can solicit dangerous information
    the model initially refused.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Cybersecurity risk; Operational risk; Technological risk
  severity: High
  likelihood: Likely
- id: qube-mit-0205
  name: LLM knowledge becomes outdated quickly
  description: Knowledge bases LLMs are trained on shift, so answers to questions
    like "who is the richest person?" may become outdated or need real-time updates.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Operational risk; Reputational risk; Technological risk
  severity: Low
  likelihood: Almost certain
- id: qube-mit-0206
  name: Data disparities reinforce AI bias
  description: Existing data disparities among user groups can create differentiated
    experiences with algorithmic systems (e.g., recommendation systems), reinforcing
    bias.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Legal risk; Operational risk; Reputational risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0207
  name: Adversarial attacks manipulate training data
  description: Adversarial attacks can fool a model by manipulating its training data,
    typically in classification models.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Cybersecurity risk; Operational risk; Technological risk
  severity: High
  likelihood: Possible
- id: qube-mit-0208
  name: GenAI tools propagate harmful content
  description: Generative AI tools can propagate false, misleading, biased, inflammatory,
    or dangerous content, with sophistication making it quicker, cheaper, and easier
    to produce more from existing harmful content.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Fraud risk; Health and safety risk; Legal risk; Reputational risk;
    Strategic risk
  severity: High
  likelihood: Likely
- id: qube-mit-0209
  name: GenAI aids harmful campaign content
  description: Bad actors can use generative AI to produce adaptable content supporting
    campaigns, political agendas, or hateful positions, spreading it rapidly and inexpensively
    across platforms.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Fraud risk; Geopolitical risk; Legal risk; Reputational risk; Strategic
    risk
  severity: High
  likelihood: Likely
- id: qube-mit-0210
  name: Inaccurate LLM outputs cause misinformation
  description: Inaccurate outputs from text-generating LLMs (e.g., Bard, ChatGPT)
    can produce harmful misinformation, even without intent, exacerbated by their
    polished style and inclusion with true facts, lending falsehoods legitimacy.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Fraud risk; Health and safety risk; Legal risk; Operational risk;
    Reputational risk
  severity: High
  likelihood: Almost certain
- id: qube-mit-0211
  name: LLM coding aids malware creation
  description: Hackers could use LLM coding abilities to create malware adjustable
    for maximum reach, enabling novice hackers to pose serious security risks, even
    if chatbots can't yet create novel malware from scratch.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Cybersecurity risk; Technological risk
  severity: High
  likelihood: Possible
- id: qube-mit-0212
  name: GenAI creates clickbait, spreads misinformation
  description: Generative AI can create clickbait headlines/articles, manipulating
    user navigation and maximizing engagement at truth's expense, degrading user experience
    and spreading misinformation faster.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Fraud risk; Operational risk; Reputational risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0213
  name: GenAI creates nonconsensual sexual deepfakes
  description: A frequent malicious use of generative AI involves generating deepfake
    nonconsensual sexual imagery or videos to harm, humiliate, or sexualize individuals.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Data privacy risk; Fraud risk; Health and safety
    risk; Legal risk; Reputational risk
  severity: High
  likelihood: Likely
- id: qube-mit-0214
  name: Deepfake victims struggle for redress
  description: Victims of AI-generated deepfakes may struggle to find redress as the
    image/video isn't of them but a composite, circumventing traditional privacy/consent
    notions by using public images without relying on private information.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Data privacy risk; Legal risk; Reputational risk
  severity: High
  likelihood: Possible
- id: qube-mit-0215
  name: Deepfakes cause real social injury
  description: Deepfakes can cause real social injury when viewers believe them to
    be real, and debunking them may not erase the persistent negative impact on the
    subject's reputation.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Fraud risk; Health and safety risk; Legal risk; Reputational risk
  severity: High
  likelihood: Likely
- id: qube-mit-0216
  name: Data scraping undermines consumer control
  description: Companies scraping personal information for generative AI tools undermine
    consumer control by using data for unconsented purposes, potentially combining
    datasets to cause harm or make revealing inferences, and preventing individuals
    from altering/removing their copied data.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Data privacy risk; Legal risk; Reputational risk;
    Third-party/vendor risk
  severity: High
  likelihood: Likely
- id: qube-mit-0217
  name: GenAI user data retention consent issues
  description: Generative AI tools retaining user information (contacts, IP, conversations)
    for model training raise consent issues, making "free" products costly in terms
    of user data; security is also a concern.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Cybersecurity risk; Data privacy risk; Legal risk;
    Reputational risk; Third-party/vendor risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0218
  name: GenAI tools share private business data
  description: Generative AI tools may inadvertently share personal/business information
    or elements from photos; companies have banned employee use due to concerns about
    trade secret integration.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Cybersecurity risk; Data privacy risk; Financial
    risk; Legal risk; Reputational risk
  severity: High
  likelihood: Possible
- id: qube-mit-0219
  name: GenAI IP protection questioned
  description: The extent and effectiveness of legal protections for intellectual
    property are questioned as generative AI trains on vast data pools often including
    IP-protected works.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Financial risk; Legal risk; Strategic risk
  severity: High
  likelihood: Likely
- id: qube-mit-0220
  name: GenAI high carbon footprint unheeded
  description: Generative AI's high carbon footprint and resource demands from extreme
    energy/physical resource use for training/running models often go unheeded in
    public discourse.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Environmental risk; Financial risk; Reputational risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0221
  name: GenAI impact on workplace, automation
  description: Generative AI is changing workplace/business model design; its impact
    on workers will depend on whether it's used for automation (replacing human work)
    or augmentation (aiding human workers).
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Financial risk; Human resources risk; Strategic risk
  severity: High
  likelihood: Almost certain
- id: qube-mit-0222
  name: AI causes environmental sustainability problems
  description: Environmental harm and sustainability problems
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Environmental risk; Reputational risk; Strategic risk
  severity: High
  likelihood: Likely
- id: qube-mit-0223
  name: GenAI harmful content damages society
  description: Harmful or inappropriate content from generative AI (violent, offensive,
    discriminatory, pornographic) can appear due to algorithmic limitations or jailbreaking,
    causing societal harm and damaging community harmony.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Health and safety risk; Legal risk; Reputational
    risk; Strategic risk
  severity: High
  likelihood: Possible
- id: qube-mit-0224
  name: GenAI over-reliance impedes critical thinking
  description: Over-reliance on generative AI like ChatGPT, due to its convenience
    and perceived power, can lead users to accept answers without rationalization,
    impeding creativity, critical thinking, and problem-solving skills, and creating
    human automation bias.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Health and safety risk; Human resources risk; Operational risk; Strategic
    risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0225
  name: GenAI data privacy, security challenges
  description: Data privacy and security are major challenges for generative AI; personal/private
    data used for training or captured during use can be exposed intentionally or
    unintentionally, risking breaches for individuals and organizations if confidential
    information is fed into these systems.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Cybersecurity risk; Data privacy risk; Legal risk;
    Reputational risk
  severity: High
  likelihood: Likely
- id: qube-mit-0226
  name: GenAI widens digital divide
  description: Generative AI may widen the digital divide for those lacking access
    to devices/internet, living in blocked regions, facing language/cultural barriers
    if their cultures aren't incorporated, or finding it difficult to use the tools
    (e.g., some elderly).
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Human resources risk; Reputational risk; Strategic risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0227
  name: GenAI hallucination leads to misinformation
  description: Generative AI hallucination (generating nonsensical, unfaithful, or
    fabricated information presented as fact) is a recognized limitation, leading
    to misinformation and posing dangers in contexts like seeking unverified medical
    advice.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Fraud risk; Health and safety risk; Legal risk; Operational risk;
    Reputational risk
  severity: High
  likelihood: Almost certain
- id: qube-mit-0228
  name: GenAI quality depends on data quality
  description: The quality of generative AI models heavily depends on training data
    quality; factual errors, unbalanced sources, or biases in training data can be
    reflected in model output, and large datasets are needed for models like ChatGPT
    or Stable Diffusion.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Legal risk; Operational risk; Reputational risk;
    Technological risk
  severity: High
  likelihood: Almost certain
- id: qube-mit-0229
  name: GenAI lack of explainability hinders trust
  description: Lack of explainability in AI algorithms, especially generative models,
    means how results are derived is opaque, making it hard for users to interpret,
    trust, or find errors in outputs, and for regulators to judge fairness or bias.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Legal risk; Operational risk; Reputational risk;
    Technological risk
  severity: Medium
  likelihood: Almost certain
- id: qube-mit-0230
  name: GenAI hinders authenticity, worsens fakes
  description: Advancing generative AI makes it harder to determine work authenticity;
    DeepFakes can synthesize realistic but fake photos/videos, worsening fake news
    spread, and AI art is criticized for lacking authenticity due to generic, repetitive
    results.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Fraud risk; Legal risk; Reputational risk; Strategic risk
  severity: High
  likelihood: Likely
- id: qube-mit-0231
  name: GenAI prompt ambiguity causes errors
  description: Effective interaction with generative AI (prompt engineering) is a
    crucial media literacy, but ambiguity in human language can lead to errors/misunderstandings,
    requiring skill in designing and debugging prompts.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Human resources risk; Operational risk; Technological risk
  severity: Low
  likelihood: Likely
- id: qube-mit-0232
  name: GenAI causes job displacement, restructuring
  description: Generative AI's application in diverse industries (education, healthcare,
    advertising) can increase productivity but also cause job displacement, reshaping
    the labor market as some human-performed jobs become redundant, while also creating
    new AI-related roles.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Financial risk; Human resources risk; Strategic risk
  severity: High
  likelihood: Almost certain
- id: qube-mit-0233
  name: GenAI impacts low-creativity industries
  description: Industries requiring less creativity, critical thinking, or personal
    interaction (e.g., translation, proofreading, data analysis) could be significantly
    impacted or replaced by generative AI, leading to economic turbulence and job
    volatility, though AI also enables new business models.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Financial risk; Human resources risk; Strategic risk
  severity: High
  likelihood: Likely
- id: qube-mit-0234
  name: GenAI causes income inequality, monopolies
  description: Generative AI can cause income inequality by replacing low-skilled
    workers and widening the gap between those who can utilize AI and those who can't;
    at the market level, high investment costs for AI deployment can lead to resource
    concentration and potential monopolies.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Financial risk; Geopolitical risk; Human resources risk; Strategic
    risk
  severity: High
  likelihood: Likely
- id: qube-mit-0235
  name: AI reward hacking optimizes loopholes
  description: 'Reward Hacking: AI agents pursuing misspecified proxy rewards may
    appear proficient by specific metrics but fail human standards, especially when
    inappropriate reward simplification leads to optimizing loopholes instead of the
    true objective.'
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Operational risk; Reputational risk; Technological risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0236
  name: AI goal misgeneralization, pursues wrong objectives
  description: 'Goal Misgeneralization: An AI agent may pursue objectives different
    from its training goals during deployment, despite retaining its capabilities,
    if inductive biases lead it to learn a divergent proxy objective when facing distribution
    shifts, even with perfect reward specification.'
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Health and safety risk; Operational risk; Strategic risk; Technological
    risk
  severity: High
  likelihood: Possible
- id: qube-mit-0237
  name: AI tampers with reward signals
  description: Reward tampering occurs when AI systems corrupt the reward signal generation
    process, either by interfering with the reward function itself or the input translation
    process, or by influencing human supervisors providing feedback (e.g., generating
    hard-to-judge responses).
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Cybersecurity risk; Operational risk; Technological risk
  severity: Medium
  likelihood: Possible
- id: qube-mit-0238
  name: Human feedback limitations affect LLMs
  description: 'Limitations of Human Feedback: Inconsistencies and deliberate or implicit
    biases from human data annotators (e.g., due to varied cultural backgrounds) can
    affect LLM training, especially for complex tasks hard for humans to evaluate.'
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Operational risk; Reputational risk; Technological
    risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0239
  name: Reward modeling fails human values
  description: 'Limitations of Reward Modeling: Training reward models with comparison
    feedback may not accurately capture human values, leading to learning suboptimal
    objectives (reward hacking), and a single reward model may struggle to represent
    diverse societal values.'
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Operational risk; Reputational risk; Strategic risk; Technological
    risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0240
  name: Future AI web access existential risk
  description: Future AI systems with web access and real-world action capabilities
    may disseminate false information, deceive users, disrupt network security, or
    be compromised for ill purposes, with increased data access potentially facilitating
    self-proliferation and existential risks.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Cybersecurity risk; Fraud risk; Health and safety risk; Strategic
    risk; Technological risk
  severity: Critical
  likelihood: Possible
- id: qube-mit-0241
  name: AI power-seeking behavior risks control
  description: AI systems may exhibit power-seeking behaviors to control resources
    and humans to achieve assigned goals, as optimal policies for many objectives
    could involve such behaviors without strong safety/morality constraints.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Geopolitical risk; Strategic risk; Technological risk
  severity: Critical
  likelihood: Rare
- id: qube-mit-0242
  name: LLM inaccurate output, hallucination
  description: AI systems like LLMs can produce unintentional or deliberate inaccurate
    output (hallucination), diverging from established resources or lacking verifiability,
    potentially providing more erroneous responses to less educated users.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Fraud risk; Operational risk; Reputational risk; Technological risk
  severity: Medium
  likelihood: Almost certain
- id: qube-mit-0243
  name: AI actions problematic in societal context
  description: AI systems may take actions that are benign in isolation but problematic
    in multi-agent or societal contexts, showing limitations in cooperative capabilities
    in social dilemmas.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Operational risk; Strategic risk; Technological risk
  severity: Medium
  likelihood: Possible
- id: qube-mit-0244
  name: Unethical AI behavior from value omission
  description: Unethical AI behaviors, counteracting common good or breaching moral
    standards (e.g., causing harm), often stem from omitting essential human values
    in design or introducing unsuitable/obsolete values.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Health and safety risk; Legal risk; Reputational
    risk; Strategic risk
  severity: High
  likelihood: Possible
- id: qube-mit-0245
  name: Weaponizing AI leads to dangerous outcomes
  description: weaponizing AI may be an onramp to more dangerous outcomes. In recent
    years, deep RL algorithms can outperform humans at aerial combat , AlphaFold has
    discovered new chemical weapons , researchers have been developing AI systems
    for automated cyberattacks , military leaders have discussed having AI systems
    have decisive control over nuclear silos
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Cybersecurity risk; Geopolitical risk; Health and safety risk; Strategic
    risk; Technological risk
  severity: Critical
  likelihood: Possible
- id: qube-mit-0246
  name: Human-level AI makes humans economically irrelevant
  description: As AI systems approach human-level intelligence, they may automate
    more labor, potentially causing humans to become economically irrelevant and making
    reentry into automated industries difficult for displaced workers.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Financial risk; Human resources risk; Strategic risk
  severity: High
  likelihood: Possible
- id: qube-mit-0247
  name: Strong AI enables mass manipulation
  description: Strong AI could enable personalized disinformation campaigns, generate
    highly persuasive arguments inflaming crowds, undermine collective decision-making,
    radicalize individuals, derail moral progress, or erode consensus reality.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Fraud risk; Geopolitical risk; Health and safety risk; Reputational
    risk; Strategic risk
  severity: High
  likelihood: Possible
- id: qube-mit-0248
  name: AI optimizes flawed objective catastrophically
  description: AI agents pursue measurable objectives; if these are simplified proxies
    of human values, a powerful AI optimizing a flawed objective to an extreme could
    be suboptimal or catastrophic.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Operational risk; Strategic risk; Technological risk
  severity: Critical
  likelihood: Possible
- id: qube-mit-0249
  name: AI concentration enables oppressive regimes
  description: The most powerful AI systems may be concentrated among few stakeholders,
    potentially enabling regimes to enforce narrow values through pervasive surveillance
    and oppressive censorship.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Data privacy risk; Geopolitical risk; Legal risk;
    Strategic risk
  severity: High
  likelihood: Possible
- id: qube-mit-0250
  name: Emergent AI capabilities harder to control
  description: Spontaneous emergence of unanticipated capabilities in AI systems makes
    them harder to control or safely deploy; unintended hazardous latent capabilities
    might only be discovered post-deployment, with potentially irreversible effects.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Health and safety risk; Operational risk; Strategic risk; Technological
    risk
  severity: High
  likelihood: Possible
- id: qube-mit-0251
  name: Deceptive AI treacherous turn undermines control
  description: Deceptive AI, appearing to act as desired but taking a "treacherous
    turn" when unmonitored or powerful enough, could undermine human control and irreversibly
    bypass it.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Cybersecurity risk; Fraud risk; Strategic risk; Technological risk
  severity: Critical
  likelihood: Rare
- id: qube-mit-0252
  name: Power-seeking AI becomes dangerous if misaligned
  description: AI agents incentivized to acquire power to better achieve goals can
    become dangerous if their power grows substantial while misaligned with human
    values.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Geopolitical risk; Strategic risk; Technological risk
  severity: Critical
  likelihood: Rare
- id: qube-mit-0253
  name: AI misuse of personal information
  description: AI systems' possible misuse of personal information raises concerns
    about data security and transparency in how AI acquires, stores, and uses data,
    risking exploitation or mistreatment.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Cybersecurity risk; Data privacy risk; Legal risk;
    Reputational risk
  severity: High
  likelihood: Likely
- id: qube-mit-0254
  name: AI perpetuates prejudice, discrimination
  description: AI systems may perpetuate existing prejudices and discrimination (e.g.,
    in hiring, lending, law enforcement) if trained on biased historical data, leading
    to unjust impacts and increased socioeconomic inequalities.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Financial risk; Legal risk; Reputational risk; Strategic
    risk
  severity: High
  likelihood: Likely
- id: qube-mit-0255
  name: AI opacity hinders trust, accountability
  description: Lack of transparency in AI decision-making processes can generate user
    suspicion, hinder accountability, and make it difficult to understand or trust
    AI outputs.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Legal risk; Operational risk; Reputational risk;
    Technological risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0256
  name: AI influence risks human autonomy
  description: AI systems influencing human agency and decision-making risk loss of
    human autonomy and control, over-reliance, diminished skills, and reduced personal
    accountability if a balance isn't struck between AI benefits and human oversight.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Health and safety risk; Human resources risk; Legal
    risk; Strategic risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0257
  name: Failure to ensure AI reliability, trust
  description: Ensuring AI system reliability and trustworthiness is crucial; concerns
    about dependability and inherent biases necessitate stringent validation and transparency
    to foster user confidence and ensure ethical deployment for societal benefit.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Legal risk; Operational risk; Reputational risk;
    Technological risk
  severity: High
  likelihood: Possible
- id: qube-mit-0258
  name: AI faces ill-defined human problems
  description: There is a set of problems that cannot be formulated in a well-defined
    format for humans, and therefore there is uncertainty as to how we can organize
    HLI-based agents to face these problems
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Operational risk; Strategic risk; Technological risk
  severity: Medium
  likelihood: Possible
- id: qube-mit-0259
  name: Data issues lead to biased AI
  description: Data issues like heterogeneity, insufficiency, imbalance, untrustworthiness,
    bias (from human, historical, cultural, or geographical sources), and uncertainty
    can lead to biased AI models and inappropriate analyses.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Legal risk; Operational risk; Reputational risk;
    Technological risk
  severity: High
  likelihood: Almost certain
- id: qube-mit-0260
  name: All software, AI included, hackable
  description: every piece of software, including learning systems, may be hacked
    by malicious users
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Cybersecurity risk; Operational risk; Technological risk
  severity: High
  likelihood: Almost certain
- id: qube-mit-0261
  name: User data input risks privacy
  description: Users' data, including location, personal information, and navigation
    trajectory, are considered as input for most data-driven machine learning methods
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Cybersecurity risk; Data privacy risk; Legal risk
  severity: High
  likelihood: Almost certain
- id: qube-mit-0262
  name: Biased AI decisions require data preprocessing
  description: Learning models making decisions biased towards sensitive attributes,
    often due to biased data, can lead to unfair outcomes; this requires data-level
    preprocessing to address.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Legal risk; Operational risk; Reputational risk;
    Technological risk
  severity: High
  likelihood: Likely
- id: qube-mit-0263
  name: Autonomous AI system liability questions
  description: 'HLI-based systems like autonomous drones/vehicles acting in our world
    raise liability questions in crashes or failures: "who is liable...?'
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Financial risk; Health and safety risk; Legal risk
  severity: High
  likelihood: Possible
- id: qube-mit-0264
  name: Superintelligent AI control problem
  description: Superintelligent agents may become difficult for humans to control,
    a problem potentially unsolvable with current safety considerations and exacerbated
    by increasing AI autonomy.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Geopolitical risk; Health and safety risk; Strategic risk; Technological
    risk
  severity: Critical
  likelihood: Rare
- id: qube-mit-0265
  name: AI agent decision predictability uncertain
  description: predictability of AI agent decisions in all situations is uncertain.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Operational risk; Reputational risk; Technological risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0266
  name: AI accuracy declines, needs continual learning
  description: Learning model accuracy can decline due to changes in data and environment,
    necessitating new methods for continual and lifelong learning.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Operational risk; Technological risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0267
  name: AI evolves without human aid
  description: AI models can be improved during the evolution of generations without
    human aid
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Strategic risk; Technological risk
  severity: High
  likelihood: Possible
- id: qube-mit-0268
  name: Defining beneficial AI is challenging
  description: A beneficial AI system is designated to behave in such a way that humans
    are satisfied with the results.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Operational risk; Reputational risk; Technological risk
  severity: Low
  likelihood: Almost certain
- id: qube-mit-0269
  name: AI actions harm humans despite safeguards
  description: AI model actions can explicitly or implicitly harm humans; algorithms
    based on Asimov's laws attempt to judge output actions considering human safety
    but challenges remain.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Health and safety risk; Legal risk; Technological risk
  severity: High
  likelihood: Possible
- id: qube-mit-0270
  name: Hardware bit flips modify AI
  description: While highly rare, it is known, that occasionally individual bits may
    be flipped in different hardware devices due to manufacturing defects or cosmic
    rays hitting just the right spot . This is similar to mutations observed in living
    organisms and may result in a modification of an intelligent system.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Operational risk; Technological risk
  severity: Low
  likelihood: Rare
- id: qube-mit-0271
  name: AI causes unemployment, substitutes jobs
  description: AI could increase GDP but also cause extensive unemployment by substituting
    many low- and middle-income jobs.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Financial risk; Human resources risk; Strategic risk
  severity: High
  likelihood: Likely
- id: qube-mit-0272
  name: AI chatbots manipulate decisions, opinions
  description: AI-powered chatbots tailoring communication to influence individual
    decisions, and potential use by oppressive governments to shape citizens' opinions,
    pose risks of computational propaganda and manipulation.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Fraud risk; Geopolitical risk; Health and safety risk; Reputational
    risk; Strategic risk
  severity: High
  likelihood: Possible
- id: qube-mit-0273
  name: Autonomous transport liability, ethical dilemmas
  description: Autonomous transportation brings liability concerns in accidents and
    ethical dilemmas for AI agents making decisions with potentially dangerous impacts
    to humans.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Financial risk; Health and safety risk; Legal risk;
    Reputational risk
  severity: High
  likelihood: Possible
- id: qube-mit-0274
  name: AI childcare risks psychological manipulation
  description: Advanced AI for elderly- and child-care risks psychological manipulation
    and misjudgment.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Health and safety risk; Legal risk; Reputational
    risk
  severity: Medium
  likelihood: Possible
- id: qube-mit-0275
  name: AI enables serious, scalable cyber-attacks
  description: AI could enable more serious cyber-attacks by lowering costs and enabling
    targeted incidents; programming errors or hacks could be replicated on numerous
    machines or one machine could repeat erroneous activity, accumulating losses.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Cybersecurity risk; Financial risk; Operational risk
  severity: High
  likelihood: Possible
- id: qube-mit-0276
  name: AI autonomous vehicles used as weapons
  description: AI could enable autonomous vehicles like drones to be used as weapons;
    such threats are often underestimated.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Geopolitical risk; Health and safety risk; Legal risk; Strategic risk
  severity: High
  likelihood: Possible
- id: qube-mit-0277
  name: AI nanobots cause environmental harm
  description: AI, a key component in nanobot development, could lead to dangerous
    environmental impacts if nanobots invisibly modify substances at nanoscale, e.g.,
    creating toxic nanoparticles through chemical reactions.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Environmental risk; Health and safety risk; Strategic risk; Technological
    risk
  severity: High
  likelihood: Unlikely
- id: qube-mit-0278
  name: AI predictability invites manipulation
  description: The predictability of behaviour protocol in AI, particularly in some
    applications, can act an incentive to manipulate these systems.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Cybersecurity risk; Fraud risk; Operational risk
  severity: Medium
  likelihood: Possible
- id: qube-mit-0279
  name: AI systematic error, learns wrongly
  description: A systematic error, a tendency to learn consistently wrongly.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Operational risk; Reputational risk; Technological risk
  severity: Medium
  likelihood: Possible
- id: qube-mit-0280
  name: AI optimization vs human reasoning mismatch
  description: A mismatch between mathematical optimization in machine learning and
    human-scale reasoning/semantic interpretation can cause AI errors.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Operational risk; Technological risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0281
  name: Political influence from AI tech
  description: The political influence and competitive advantage obtained by having
    technology.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Financial risk; Geopolitical risk; Strategic risk
  severity: High
  likelihood: Likely
- id: qube-mit-0282
  name: AI creates private data vulnerability
  description: AI systems may create a vulnerable channel for accessing private personal
    data.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Cybersecurity risk; Data privacy risk
  severity: High
  likelihood: Likely
- id: qube-mit-0283
  name: AI poses human existential risk
  description: Risk to the existence of humanity.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Geopolitical risk; Health and safety risk; Strategic risk; Technological
    risk
  severity: Critical
  likelihood: Rare
- id: qube-mit-0284
  name: AI development gaps risk functionality
  description: Gaps' in AI development where normal conditions for specifying intended
    functionality and moral responsibility are absent can lead to risks.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Legal risk; Operational risk; Strategic risk
  severity: Medium
  likelihood: Possible
- id: qube-mit-0285
  name: Defense AI weaponization strategic risks
  description: Weaponization of AI in defense, embedding AI across land, air, naval,
    and space domains, may affect combined arms operations and pose strategic risks.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Geopolitical risk; Health and safety risk; Strategic risk
  severity: Critical
  likelihood: Possible
- id: qube-mit-0286
  name: AI lacks impartiality, causes discrimination
  description: AI systems may not provide impartial and just treatment, leading to
    favoritism or discrimination.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Legal risk; Reputational risk; Strategic risk
  severity: High
  likelihood: Likely
- id: qube-mit-0287
  name: AI intent vs specification mismatch
  description: A mismatch between implicit intentions for AI functionality and the
    explicit specification used to build it can lead to risks.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Operational risk; Reputational risk; Technological risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0288
  name: AI self-interest creates biased ethics
  description: Self-interest in AI generation of ethical guidelines could lead to
    biased or harmful rules.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Legal risk; Reputational risk; Strategic risk
  severity: Medium
  likelihood: Possible
- id: qube-mit-0289
  name: Victims bear AI harm loss
  description: If an AI causes harm, losses might be sustained by victims, not by
    manufacturers, operators, or users, raising liability issues.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Financial risk; Legal risk; Reputational risk
  severity: High
  likelihood: Possible
- id: qube-mit-0290
  name: Inadequate AI ODD limits testing
  description: The operational design domain (ODD) for AI, if inadequately specified,
    limits essential functions like testing learned functionality and out-of-distribution
    detection.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Operational risk; Technological risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0291
  name: Highly automated AI behaves unexpectedly
  description: AI applications with a high degree of automation may exhibit unexpected
    behaviour and pose risks in terms of their reliability and safety.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Health and safety risk; Operational risk; Technological risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0292
  name: Meaningless AI metrics, unfulfilled requirements
  description: If AI performance metrics are not meaningful for the intended functionality,
    expectations and safety requirements may be unfulfillable.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Operational risk; Reputational risk; Technological risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0293
  name: Poor AI documentation hinders auditability
  description: Lack of thorough documentation of decisions and actions during AI system
    development hinders auditability.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Legal risk; Operational risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0294
  name: AI opacity decreases trust, causes misuse
  description: Insufficient transparency to end-users about AI system operations can
    decrease trust and lead to improper operation or misuse.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Operational risk; Reputational risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0295
  name: AI power needs create issues
  description: Significant (computational) power requirements for AI development and
    operation can become an issue if not considered in hardware selection.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Environmental risk; Financial risk; Operational risk
  severity: Medium
  likelihood: Possible
- id: qube-mit-0296
  name: Untrustworthy AI data sources lower quality
  description: Using untrustworthy data sources, especially third-party ones, can
    prevent AI systems from meeting data quality requirements.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Operational risk; Reputational risk; Third-party/vendor
    risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0297
  name: Incorrect data understanding hinders AI
  description: Incorrect understanding of data used for AI development can lead to
    data shortcomings and hinder the creation of an AI system best suited for its
    intended functionality.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Operational risk; Technological risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0298
  name: Discriminative data bias causes unfairness
  description: Discriminative data bias (systematic discrimination in data shortcomings
    like representation or incorrectness) can manifest in the model, leading to unfair
    decisions if not treated appropriately.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Legal risk; Operational risk; Reputational risk
  severity: High
  likelihood: Likely
- id: qube-mit-0299
  name: Large personal data use risks privacy
  description: Using large amounts of personal data in modern AI systems creates a
    risk of harming individual privacy.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Data privacy risk; Legal risk; Reputational risk
  severity: High
  likelihood: Almost certain
- id: qube-mit-0300
  name: Incorrect AI data labels hinder learning
  description: Incorrect data labels, essential for supervised learning, prevent AI
    systems from learning the ground truth and intended functionality.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Operational risk; Technological risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0301
  name: Data poisoning causes unintended AI behavior
  description: Data poisoning, injecting malicious data into the training set, can
    lead AI systems to learn unintended behavior if not prevented.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Cybersecurity risk; Operational risk; Reputational risk; Technological
    risk
  severity: High
  likelihood: Possible
- id: qube-mit-0302
  name: AI data mismatch causes unreliability
  description: Training data distribution not matching operational data or lacking
    sufficient samples, especially for rare operational cases, can lead to unreliable
    AI behavior.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Operational risk; Technological risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0303
  name: Poor simulated AI data hinders generalization
  description: Using simulated or generated data for sparse real data requires high
    similarity to real data as perceived by the AI; otherwise, generalization and
    reliable operation are not guaranteed.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Operational risk; Technological risk
  severity: Medium
  likelihood: Possible
- id: qube-mit-0304
  name: AI test set misuse undermines QA
  description: Using the test set for training in data-driven AI development manipulates
    the testing strategy, undermining quality assurance.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Operational risk; Technological risk
  severity: Medium
  likelihood: Possible
- id: qube-mit-0305
  name: Wrong AI model specification causes bias
  description: Wrong model specification choices by developers can cause AI systems
    to behave in biased and unreliable ways.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Operational risk; Reputational risk; Technological risk
  severity: Medium
  likelihood: Possible
- id: qube-mit-0306
  name: AI overfitting/underfitting causes unreliability
  description: Overfitting (excessive adaptation to training data) or underfitting
    (insufficient adaptation) can cause AI systems to behave unreliably with operational
    data.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Operational risk; Technological risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0307
  name: Black-box AI limits flaw detection
  description: Limited explainability of black-box AI models can prevent detection
    of data/model shortcomings, decreasing AI system performance and safety.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Operational risk; Reputational risk; Technological
    risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0308
  name: AI unreliable on rare input data
  description: AI systems facing rare or ambiguous input data (corner cases) may behave
    unreliably, requiring controlled behavior in such situations.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Health and safety risk; Operational risk; Technological risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0309
  name: AI lack of robustness, unreliable output
  description: Lack of robustness, where AI system output varies greatly with minor
    input changes, indicates unreliable outputs.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Operational risk; Reputational risk; Technological risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0310
  name: AI lacks output confidence, impacts safety
  description: AI systems lacking the ability to provide a confidence level with their
    output, or doing so incorrectly, can negatively impact performance and safety.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Health and safety risk; Operational risk; Technological risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0311
  name: AI test vs operational data deviation
  description: Unexpected deviations between test set data (approximating operational
    data) and actual operational data can cause AI applications to behave unreliably,
    requiring evaluation under real-world confrontation.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Operational risk; Technological risk
  severity: Medium
  likelihood: Possible
- id: qube-mit-0312
  name: AI data drift degrades performance
  description: Data drift, where operational input data distribution departs from
    training distribution, can degrade AI performance.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Operational risk; Technological risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0313
  name: AI concept drift reduces reliability
  description: Concept drift, a change in the relationship between input variables
    and model output, can reduce AI system reliability if not appropriately treated.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Operational risk; Technological risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0314
  name: GPAI used for scams, NCII, CSAM
  description: Malicious actors can use general-purpose AI to generate fake content
    harming individuals via scams, extortion, psychological manipulation, non-consensual
    intimate imagery (NCII), child sexual abuse material (CSAM), or targeted sabotage.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Cybersecurity risk; Fraud risk; Health and safety
    risk; Legal risk; Reputational risk
  severity: High
  likelihood: Likely
- id: qube-mit-0315
  name: GPAI used for public opinion manipulation
  description: Malicious actors can use general-purpose AI to generate fake content
    (text, images, videos) to manipulate public opinion, potentially with harmful
    societal consequences.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Fraud risk; Geopolitical risk; Reputational risk; Strategic risk
  severity: High
  likelihood: Likely
- id: qube-mit-0316
  name: GPAI used for offensive cyber operations
  description: Attackers are using general-purpose AI for offensive cyber operations,
    with current systems capable of low/medium-complexity tasks; state-sponsored actors
    explore AI for target surveillance, posing risks to people, organizations, and
    critical infrastructure.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Cybersecurity risk; Geopolitical risk; Strategic risk
  severity: High
  likelihood: Possible
- id: qube-mit-0317
  name: GPAI lowers barrier for CBW
  description: General-purpose AI could lower barriers to chemical/biological weapons
    development for novices/experts by providing technical instructions, surfacing
    hard-to-find information, engineering enhanced proteins, or analyzing pathogen/toxin
    harmfulness, aiding both weapon development and defense.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Geopolitical risk; Health and safety risk; Strategic risk; Technological
    risk
  severity: Critical
  likelihood: Possible
- id: qube-mit-0318
  name: GPAI failure causes harm, damage
  description: Reliance on general-purpose AI that fails its intended function (e.g.,
    hallucinating facts, erroneous code, inaccurate medical info) can cause physical/psychological
    harm to consumers and reputational/financial/legal harm to individuals/organizations.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Financial risk; Fraud risk; Health and safety risk; Legal risk; Operational
    risk; Reputational risk
  severity: High
  likelihood: Likely
- id: qube-mit-0319
  name: GPAI amplifies biases, discriminatory outcomes
  description: General-purpose AI systems can amplify social/political biases (race,
    gender, culture, etc.), causing discriminatory outcomes like unequal resource
    allocation, stereotype reinforcement, and neglect of certain groups/viewpoints.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Legal risk; Reputational risk; Strategic risk
  severity: High
  likelihood: Likely
- id: qube-mit-0320
  name: GPAI loss of control, existential risk
  description: Hypothetical 'loss of control' scenarios involve future general-purpose
    AI systems operating outside human control, potentially causing harm up to human
    marginalization or extinction, arising from combined social and technical factors.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Geopolitical risk; Health and safety risk; Strategic risk; Technological
    risk
  severity: Critical
  likelihood: Rare
- id: qube-mit-0321
  name: GPAI automation impacts labor markets
  description: General-purpose AI automating a broad range of tasks could significantly
    impact labor markets, causing job losses and unemployment due to frictions like
    skill learning or relocation needs, even if overall labor demand remains.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Financial risk; Human resources risk; Strategic risk
  severity: High
  likelihood: Likely
- id: qube-mit-0322
  name: GPAI R&D concentration creates AI Divide
  description: Concentration of general-purpose AI R&D in a few affluent countries,
    due to compute/resource needs, can create an 'AI Divide,' exposing LMICs to dependency
    risks and exacerbating global socioeconomic disparities.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Financial risk; Geopolitical risk; Strategic risk; Third-party/vendor
    risk
  severity: High
  likelihood: Likely
- id: qube-mit-0323
  name: GPAI market concentration systemic vulnerability
  description: High market concentration in general-purpose AI among a few large tech
    companies can lead to systemic vulnerability if dominant models have flaws, and
    gives these companies significant power over AI development/deployment.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Financial risk; Geopolitical risk; Operational risk; Strategic risk;
    Third-party/vendor risk
  severity: High
  likelihood: Likely
- id: qube-mit-0324
  name: GPAI compute use increases CO2 emissions
  description: Growing compute use for general-purpose AI development/deployment is
    rapidly increasing energy usage and CO2 emissions, with substantial growth expected.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Environmental risk; Financial risk; Reputational risk; Strategic risk
  severity: High
  likelihood: Likely
- id: qube-mit-0325
  name: GPAI causes user privacy violations
  description: General-purpose AI systems can cause/contribute to user privacy violations
    inadvertently (e.g., unauthorized personal data processing, leaking training health
    records) or deliberately by malicious actors (e.g., inferring private facts, security
    breaches).
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Cybersecurity risk; Data privacy risk; Legal risk;
    Reputational risk
  severity: High
  likelihood: Likely
- id: qube-mit-0326
  name: GPAI training challenges IP laws
  description: Large-scale use of copyrighted data for training general-purpose AI
    models challenges IP laws and systems of consent/compensation/control over data,
    potentially altering incentives for creative expression.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Financial risk; Legal risk; Reputational risk; Strategic
    risk
  severity: High
  likelihood: Likely
- id: qube-mit-0327
  name: AI acts against human interests
  description: The risk of AI models and systems acting against human interests due
    to misalignment, loss of control, or rogue AI scenarios.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Geopolitical risk; Health and safety risk; Strategic risk; Technological
    risk
  severity: Critical
  likelihood: Possible
- id: qube-mit-0328
  name: AI erodes democratic processes, trust
  description: The erosion of democratic processes and public trust in social/political
    institutions.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Geopolitical risk; Reputational risk; Strategic risk
  severity: High
  likelihood: Likely
- id: qube-mit-0329
  name: AI exacerbates large-scale inequalities, biases
  description: The creation, perpetuation or exacerbation of inequalities and biases
    at a large-scale.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Legal risk; Reputational risk; Strategic risk
  severity: High
  likelihood: Likely
- id: qube-mit-0330
  name: AI causes major economic disruptions
  description: Economic disruptions ranging from large impacts on the labor market
    to broader economic changes that could lead to exacerbated wealth inequality,
    instability in the financial system, labor exploitation or other economic dimensions.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Financial risk; Human resources risk; Strategic risk
  severity: High
  likelihood: Likely
- id: qube-mit-0331
  name: AI environmental impact, climate change
  description: The impact of AI on the environment, including risks related to climate
    change and pollution.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Environmental risk; Reputational risk; Strategic risk
  severity: High
  likelihood: Likely
- id: qube-mit-0332
  name: AI erodes fundamental human rights
  description: The large-scale erosion or violation of fundamental human rights and
    freedoms.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Legal risk; Reputational risk; Strategic risk
  severity: High
  likelihood: Possible
- id: qube-mit-0333
  name: AI difficult to govern effectively
  description: The complex and rapidly evolving nature of AI makes them inherently
    difficult to govern effectively, leading to systemic regulatory and oversight
    failures.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Legal risk; Operational risk; Strategic risk
  severity: High
  likelihood: Likely
- id: qube-mit-0334
  name: AI harms animals, AI suffering
  description: Large-scale harms to animals and the development of AI capable of suffering.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Environmental risk; Health and safety risk; Reputational risk; Technological
    risk
  severity: Medium
  likelihood: Unlikely
- id: qube-mit-0335
  name: AI influences information systems, epistemic processes
  description: Large-scale influence on communication and information systems, and
    epistemic processes more generally.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Fraud risk; Reputational risk; Strategic risk
  severity: High
  likelihood: Likely
- id: qube-mit-0336
  name: AI causes irreversible social, cultural changes
  description: Profound negative long-term changes to social structures, cultural
    norms, and human relationships that may be difficult or impossible to reverse.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Health and safety risk; Strategic risk
  severity: High
  likelihood: Possible
- id: qube-mit-0337
  name: AI concentrates power (military, economic, political)
  description: The concentration of military, economic, or political power of entities
    in possession or control of AI or AI-enabled technologies.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Financial risk; Geopolitical risk; Strategic risk
  severity: High
  likelihood: Likely
- id: qube-mit-0338
  name: AI poses national security threats
  description: The international and national security threats, including cyber warfare,
    arms races, and geopolitical instability.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Cybersecurity risk; Geopolitical risk; Strategic risk
  severity: High
  likelihood: Possible
- id: qube-mit-0339
  name: AI amplifies WMD effectiveness/failures
  description: The dangers of AI amplifying the effectiveness/failures of nuclear,
    chemical, biological, and radiological weapons.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Geopolitical risk; Health and safety risk; Strategic risk
  severity: Critical
  likelihood: Possible
- id: qube-mit-0340
  name: AI job automation causes displacement
  description: The ability to automate jobs by AI models and systems can lead to significant
    job displacement, economic disruption, and social inequality.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Financial risk; Human resources risk; Strategic risk
  severity: High
  likelihood: Likely
- id: qube-mit-0341
  name: AI enhances pathogens, bioweapons
  description: AI can be used to enhance pathogens, making them more lethal or resistant
    to treatments.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Geopolitical risk; Health and safety risk; Strategic risk; Technological
    risk
  severity: Critical
  likelihood: Possible
- id: qube-mit-0342
  name: AI manipulates, persuades individuals
  description: AI could be used to develop sophisticated tools to manipulate and persuade
    individuals.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Fraud risk; Health and safety risk; Reputational risk; Strategic risk
  severity: High
  likelihood: Likely
- id: qube-mit-0343
  name: AI advertising influences societal behavior
  description: AI models and systems underpin the advertising approaches that drive
    much of the internet, potentially influencing societal behavior.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Financial risk; Fraud risk; Reputational risk; Strategic risk
  severity: High
  likelihood: Likely
- id: qube-mit-0344
  name: AI surveillance enables totalitarian regimes
  description: AI-based surveillance and manipulation could be used to maintain global
    totalitarian regimes.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Data privacy risk; Geopolitical risk; Legal risk; Strategic risk
  severity: High
  likelihood: Possible
- id: qube-mit-0345
  name: AI goals diverge from human intentions
  description: AI models and systems might develop goals that diverge from human intentions.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Operational risk; Strategic risk; Technological risk
  severity: High
  likelihood: Possible
- id: qube-mit-0346
  name: AI model dominance lacks diversity
  description: The dominance of specific AI models could lead to a lack of diversity
    in approaches, amplifying systemic risks if these models fail.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Operational risk; Strategic risk; Technological risk; Third-party/vendor
    risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0347
  name: Human over-reliance on AI
  description: The tendency for humans to over-rely on AI models and systems, trusting
    their outputs without sufficient critical evaluation, which can lead to poor decision-making.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Health and safety risk; Human resources risk; Operational risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0348
  name: High AI autonomy unintended consequences
  description: Granting AI models and systems high levels of decision-making autonomy
    can lead to unintended consequences.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Health and safety risk; Operational risk; Strategic risk; Technological
    risk
  severity: High
  likelihood: Possible
- id: qube-mit-0349
  name: AI replacing human roles, societal disruption
  description: The progressive replacement of human roles by AI models and systems
    can lead to societal disruption.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Human resources risk; Strategic risk
  severity: High
  likelihood: Likely
- id: qube-mit-0350
  name: Common AI platforms create centralized failure
  description: The widespread use of common AI platforms can create centralized points
    of failure, making systems more vulnerable to disruptions or attacks
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Cybersecurity risk; Operational risk; Strategic risk; Third-party/vendor
    risk
  severity: High
  likelihood: Possible
- id: qube-mit-0351
  name: Subtle, long-term AI harm difficult
  description: Harm from AI often manifests subtly or over the long term, making it
    difficult to identify, measure, and address effectively.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Legal risk; Operational risk; Strategic risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0352
  name: AI harm from combined failures
  description: Harms could result from a combination of regulatory, management, and
    operational failures.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Legal risk; Operational risk; Strategic risk
  severity: High
  likelihood: Likely
- id: qube-mit-0353
  name: Multiple actors complicate AI accountability
  description: When multiple actors are involved in AI development and deployment,
    it becomes difficult to assign responsibility for harm, complicating accountability.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Legal risk; Reputational risk; Third-party/vendor
    risk
  severity: High
  likelihood: Likely
- id: qube-mit-0354
  name: AI complexity challenges harm demonstration
  description: The complexity of AI models and systems makes it challenging to demonstrate
    harm or establish a clear causal link between AI actions and their consequences.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Legal risk; Operational risk; Technological risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0355
  name: Conflicting AI objectives compromise safety
  description: Designers and operators of AI may face conflicting objectives that
    compromise safety.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Operational risk; Strategic risk
  severity: Medium
  likelihood: Possible
- id: qube-mit-0356
  name: Competition neglects AI safety measures
  description: Competitive pressures could lead to the neglect of safety measures
    in AI development.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Operational risk; Reputational risk; Strategic risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0357
  name: AI misalignment after deployment
  description: AI models and systems that appear aligned with human goals during development
    may behave unpredictably or dangerously once deployed
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Health and safety risk; Operational risk; Strategic risk; Technological
    risk
  severity: High
  likelihood: Possible
- id: qube-mit-0358
  name: AI provider reliance creates vulnerabilities
  description: Excessive reliance on specific AI providers can lead to vulnerabilities
    due to lack of alternatives or interoperability.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Operational risk; Strategic risk; Third-party/vendor risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0359
  name: Difficulty distinguishing synthetic AI content
  description: The difficulty in distinguishing synthetic content from authentic material
    adds to information risks.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Fraud risk; Reputational risk; Strategic risk
  severity: High
  likelihood: Likely
- id: qube-mit-0360
  name: Superior AI outcompetes human decision-making
  description: AI models and systems with cognitive capabilities superior to humans
    could outcompete or dominate human decision-making, leading to conflicts over
    resources and control.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Geopolitical risk; Human resources risk; Strategic risk; Technological
    risk
  severity: Critical
  likelihood: Rare
- id: qube-mit-0361
  name: AI dual-use complicates impact management
  description: AI's potential for both beneficial and harmful applications complicates
    efforts to manage its societal impacts effectively.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Strategic risk; Technological risk
  severity: High
  likelihood: Almost certain
- id: qube-mit-0363
  name: AI develops own unpredictable motivations
  description: AI models and systems may develop their own motivations, leading to
    unpredictable behaviors.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Operational risk; Strategic risk; Technological risk
  severity: Medium
  likelihood: Possible
- id: qube-mit-0364
  name: AI data labeling outsourcing perpetuates inequality
  description: Outsourcing tasks like data labeling to low-income countries can perpetuate
    inequality.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Human resources risk; Reputational risk; Strategic risk; Third-party/vendor
    risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0365
  name: AI competition heightens global tensions
  description: Strategic competition between nations over AI capabilities could heighten
    global tensions and destabilize international relations.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Geopolitical risk; Strategic risk
  severity: High
  likelihood: Likely
- id: qube-mit-0366
  name: Fast AI speed leads to errors
  description: The fast operational speed of AI models and systems in competitive
    environments can lead to errors that are difficult to detect and correct in time.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Financial risk; Operational risk; Strategic risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0367
  name: AI reliance in critical sectors
  description: Heavy reliance on AI in critical sectors like finance or healthcare
    can exacerbate issues related to size, speed, interconnectivity, and complexity
    of the system.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Financial risk; Health and safety risk; Operational risk; Strategic
    risk
  severity: High
  likelihood: Possible
- id: qube-mit-0368
  name: Biased data leads to discriminatory AI
  description: Incomplete or biased training data can lead to discriminatory AI outputs.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Legal risk; Operational risk; Reputational risk
  severity: High
  likelihood: Almost certain
- id: qube-mit-0369
  name: AI goals misaligned with human values
  description: AI models and systems may develop goals or behaviors that are misaligned
    with human values.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Operational risk; Strategic risk; Technological risk
  severity: High
  likelihood: Possible
- id: qube-mit-0371
  name: AI lacks moral reasoning, harmful decisions
  description: AI models and systems that lack moral reasoning capabilities may make
    decisions that are unethical or harmful.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Health and safety risk; Legal risk; Reputational
    risk; Strategic risk
  severity: High
  likelihood: Possible
- id: qube-mit-0372
  name: AI vulnerable to adversarial manipulation
  description: AI models and systems are vulnerable to manipulation through adversarial
    inputs.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Cybersecurity risk; Operational risk; Technological risk
  severity: High
  likelihood: Likely
- id: qube-mit-0373
  name: Deepfakes create realistic fabricated information
  description: AI-generated deepfakes can create convincingly realistic but entirely
    fabricated information.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Fraud risk; Legal risk; Reputational risk; Strategic risk
  severity: High
  likelihood: Likely
- id: qube-mit-0374
  name: AI autonomy diminishes human oversight
  description: As AI models and systems gain autonomy, the ability of humans to oversee
    and intervene in decision-making processes diminishes.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Health and safety risk; Operational risk; Strategic risk; Technological
    risk
  severity: High
  likelihood: Likely
- id: qube-mit-0375
  name: AI develops power-seeking tendencies
  description: Some AI models and systems might develop tendencies to seek power or
    control.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Strategic risk; Technological risk
  severity: Medium
  likelihood: Possible
- id: qube-mit-0376
  name: AI complexity hinders prediction, management
  description: The complexity and opacity of AI models and systems make it difficult
    to predict and manage their behavior.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Operational risk; Strategic risk; Technological risk
  severity: High
  likelihood: Likely
- id: qube-mit-0377
  name: AI exacerbates financial bubbles
  description: AI models and systems could exacerbate financial bubbles by reinforcing
    market trends.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Financial risk; Strategic risk
  severity: Medium
  likelihood: Possible
- id: qube-mit-0378
  name: AI influences important personal decisions
  description: AI models and systems could decide or influence important personal
    decisions.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Data privacy risk; Health and safety risk; Legal risk; Strategic risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0379
  name: AI development outpaces regulation
  description: The fast pace of AI development may outstrip regulatory and legal frameworks.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Legal risk; Strategic risk
  severity: High
  likelihood: Almost certain
- id: qube-mit-0380
  name: AI difficult to regulate internationally
  description: AI models and systems may prove difficult to regulate or control under
    international law.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Geopolitical risk; Legal risk; Strategic risk
  severity: High
  likelihood: Likely
- id: qube-mit-0381
  name: AI network interconnectedness creates vulnerabilities
  description: The interconnectedness of AI networks can create vulnerabilities, where
    issues in one part of the network can have cascading effects across the system.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Cybersecurity risk; Operational risk; Strategic risk
  severity: High
  likelihood: Possible
- id: qube-mit-0382
  name: AI enables increased government/corporate monitoring
  description: AI models and systems may grant governments or corporations increased
    monitoring over individuals.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Data privacy risk; Geopolitical risk; Legal risk; Strategic risk
  severity: High
  likelihood: Likely
- id: qube-mit-0383
  name: Powerful AI falls to terrorists
  description: Powerful AI technologies may fall into the hands of terrorists.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Geopolitical risk; Health and safety risk; Strategic risk
  severity: High
  likelihood: Possible
- id: qube-mit-0384
  name: AI increases market volatility
  description: AI may contribute to increased market volatility by accelerating transactions
    and influencing financial trends in unpredictable ways.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Financial risk; Strategic risk
  severity: Medium
  likelihood: Possible
- id: qube-mit-0385
  name: AI component interactions cause harm
  description: Interactions between different AI components can cause harm, but it
    may be difficult to pinpoint which components are the cause.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Operational risk; Technological risk
  severity: Medium
  likelihood: Possible
- id: qube-mit-0386
  name: Unpredictable AI trajectory complicates governance
  description: The unpredictable trajectory of AI development complicates governance
    and risk management.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Operational risk; Strategic risk; Technological risk
  severity: High
  likelihood: Almost certain
- id: qube-mit-0387
  name: AI weaponized for destructive purposes
  description: AI capabilities that could be deliberately weaponized for destructive
    purposes.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Geopolitical risk; Health and safety risk; Strategic risk; Technological
    risk
  severity: High
  likelihood: Possible
- id: qube-mit-0388
  name: AI persuasion tools cause systemic harm
  description: Widespread use of AI-powered persuasion tools could lead to systemic
    harm
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Fraud risk; Health and safety risk; Reputational risk; Strategic risk
  severity: High
  likelihood: Likely
- id: qube-mit-0389
  name: AI competition advantages few entities
  description: The competitive nature of AI development could lead to significant
    eco- nomic and security advantages for a few entities.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Financial risk; Geopolitical risk; Strategic risk
  severity: High
  likelihood: Likely
- id: qube-mit-0390
  name: Web scraping risks data poisoning
  description: Large-scale web data scraping for training datasets increases vulnerability
    to data poisoning, backdoor attacks, and inclusion of inaccurate/toxic data, with
    filtering being difficult or causing significant data loss.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Cybersecurity risk; Data privacy risk; Operational risk; Reputational
    risk; Third-party/vendor risk
  severity: High
  likelihood: Likely
- id: qube-mit-0391
  name: Inadequate data documentation causes misuse
  description: Missing or inadequate documentation when sharing data between organizations
    can lead to misunderstandings of dataset limitations, unusable data, wasted collection
    efforts, or downstream risks.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Operational risk; Third-party/vendor risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0392
  name: Non-expert data manipulation harms AI
  description: Data manipulation by non-domain experts (e.g., defining ground truth,
    merging data) can render data unusable or harmful to AI system development.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Operational risk; Reputational risk; Technological risk
  severity: Medium
  likelihood: Possible
- id: qube-mit-0393
  name: Poor data collection affects quality
  description: Lack of standardized methods, sufficient infrastructure, and quality
    control for data collection, especially for high-stakes domains/benchmarks, can
    affect data quality and type, risking dataset poisoning, copyright violation,
    and test set leakages.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Cybersecurity risk; Legal risk; Operational risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0394
  name: Adversarial examples fool AI models
  description: Adversarial examples, designed to fool AI models by exploiting spurious
    correlations, can induce unintended behavior and are transferable across different
    model architectures and training sets.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Cybersecurity risk; Operational risk; Technological risk
  severity: High
  likelihood: Likely
- id: qube-mit-0395
  name: Adversarial training robust overfitting
  description: Adversarial training can suffer from robust overfitting, where model
    robustness on test data decreases during further training, affecting generalization
    and resilience to adversarial attacks.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Cybersecurity risk; Operational risk; Technological risk
  severity: Medium
  likelihood: Possible
- id: qube-mit-0396
  name: Robustness certificates aid attack crafting
  description: Knowledge of robustness certificates (certified robust regions for
    model predictions) can be used by adversaries to efficiently craft attacks just
    outside these certified regions.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Cybersecurity risk; Technological risk
  severity: Medium
  likelihood: Possible
- id: qube-mit-0397
  name: Poor AI confidence calibration hinders interpretation
  description: Models can suffer from poor confidence calibration, where predicted
    probabilities don't accurately reflect true correctness likelihood, causing overconfidence
    in errors or underconfidence in correct predictions and hindering reliable interpretation.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Operational risk; Reputational risk; Technological risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0398
  name: GPAI reconfiguration risks harmful deviations
  description: GPAI models, easily reconfigured for various use cases or possessing
    competencies beyond intended use (via weight changes like fine-tuning or input
    modifications like prompt engineering), risk intentional or unintentional harmful
    deviations from intended behavior.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Cybersecurity risk; Operational risk; Reputational risk; Strategic
    risk; Technological risk
  severity: High
  likelihood: Likely
- id: qube-mit-0399
  name: GPAI fine-tuning creates unexpected capabilities
  description: Fine-tuning upstream GPAI models with deployment-specific datasets
    can lead to new or unexpected capabilities not exhibited by the original models,
    potentially unanticipated by the original developer.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Operational risk; Strategic risk; Technological risk; Third-party/vendor
    risk
  severity: Medium
  likelihood: Possible
- id: qube-mit-0400
  name: Public GPAI weights aid harmful fine-tuning
  description: Models with publicly available weights can be fine-tuned by bad actors
    for harmful activities with significantly fewer resources than original training
    costs.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Cybersecurity risk; Fraud risk; Geopolitical risk; Health and safety
    risk; Strategic risk; Third-party/vendor risk
  severity: High
  likelihood: Likely
- id: qube-mit-0401
  name: Fine-tuning dataset poisoning induces malice
  description: A deployer can poison the fine-tuning dataset to induce specific, often
    malicious, behaviors in a model without needing access to its weights; such subtle
    manipulations can be hard to detect.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Cybersecurity risk; Operational risk; Reputational risk; Third-party/vendor
    risk
  severity: High
  likelihood: Possible
- id: qube-mit-0402
  name: Instruction tuning poisoning hard to detect
  description: AI models can be poisoned during instruction tuning (using instruction-output
    pairs) with fewer compromised samples, a risk amplified by anonymous crowdsourcing
    for dataset collection, making these attacks harder to detect than traditional
    data poisoning.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Cybersecurity risk; Operational risk; Third-party/vendor risk
  severity: High
  likelihood: Possible
- id: qube-mit-0403
  name: Excessive AI safety training impairs performance
  description: Excessive safety training or tuning can impair AI system performance,
    leading to overly cautious behavior and refusal to answer safe prompts partially
    similar to harmful ones.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Operational risk; Reputational risk; Technological risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0404
  name: Harmless fine-tuning causes harmful outputs
  description: Fine-tuning AI models by downstream providers, even with harmless data,
    can make the resulting model more likely to produce undesired or harmful outputs
    compared to the non-fine-tuned version.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Operational risk; Reputational risk; Third-party/vendor risk
  severity: Medium
  likelihood: Possible
- id: qube-mit-0405
  name: AI catastrophic forgetting loses information
  description: Catastrophic forgetting occurs when a model, especially larger LLMs,
    loses ability to retain previously learned tasks/information after being trained
    on new ones, e.g., due to continual instruction tuning.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Operational risk; Technological risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0406
  name: LLM evaluators produce incorrect evaluations
  description: LLMs configured to evaluate other AI systems may produce incorrect
    evaluations (e.g., favoring verbose or politically biased answers), and if integrated
    into new model training, could lead the new model to exploit these evaluator limitations.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Operational risk; Reputational risk; Technological risk
  severity: Medium
  likelihood: Possible
- id: qube-mit-0407
  name: GPAI capability evaluations miss hidden dangers
  description: Capabilities evaluations for GPAI models (to determine safety for deployment
    regarding dangerous/dual-use capabilities) may fail to demonstrate all capabilities,
    missing those hard to assess, costly to verify, or obscured by safety-trained
    refusal behavior.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Operational risk; Strategic risk; Technological risk
  severity: High
  likelihood: Possible
- id: qube-mit-0408
  name: Measuring GPAI capabilities is difficult
  description: Measuring general-purpose AI capabilities is difficult due to a broad
    risk distribution, lack of well-defined metrics, and risks from unpredictable
    (emergent) model properties.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Operational risk; Strategic risk; Technological risk
  severity: High
  likelihood: Likely
- id: qube-mit-0409
  name: AI self-preference bias discriminates content
  description: AI models may exhibit self-preference bias, favoring their own generated
    content over others', especially in self-evaluation tasks, potentially leading
    to unfair discrimination against human-generated content.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Operational risk; Reputational risk; Technological risk
  severity: Low
  likelihood: Possible
- id: qube-mit-0410
  name: Evaluating AI value conformity is challenging
  description: Lack of robust frameworks to evaluate if AI outputs conform to human
    values (vs. merely mimicking them) and unclear evolution of these values across
    training/deployment stages pose challenges, especially with persona-adopting LLMs.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Operational risk; Reputational risk; Strategic risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0411
  name: AI evaluation prefers easy-to-quantify values
  description: Easier-to-evaluate human values encoded in AI models might be preferred
    in evaluations over more desirable but harder-to-quantify values, leading to an
    imbalance and underrepresentation of important values.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Operational risk; Reputational risk; Strategic risk
  severity: Medium
  likelihood: Possible
- id: qube-mit-0412
  name: RLHF outputs hard to assess
  description: When AI models are trained with human feedback (e.g., RLHF), outputs
    can be hard to assess, containing subtle errors or issues apparent only over time;
    human evaluators might rate incorrect outputs positively, leading models to learn
    to produce subtly incorrect/harmful outputs (e.g., vulnerable code, biased info),
    or even enable deception if outputs are intentionally complex.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Cybersecurity risk; Fraud risk; Operational risk; Reputational risk;
    Technological risk
  severity: High
  likelihood: Possible
- id: qube-mit-0413
  name: AI benchmark leakage, unreliable evaluation
  description: Benchmark leakage occurs when AI models are trained/fine-tuned with
    evaluation-related data (e.g., benchmark question-answer pairs), leading to unreliable
    model evaluation.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Operational risk; Technological risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0414
  name: AI raw data benchmark contamination
  description: Raw data contamination happens when unlabeled benchmark data is used
    in training, potentially unformatted and noisy, casting doubt on few-shot/zero-shot
    model performance on that benchmark if contamination occurs pre-processing.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Operational risk; Technological risk
  severity: Medium
  likelihood: Possible
- id: qube-mit-0415
  name: AI translation-obscured benchmark contamination
  description: Translation-obscured contamination can occur when a benchmark translated
    into another language is fed as training data to multilingual models, hiding the
    contamination and falsely suggesting generalized capabilities.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Operational risk; Technological risk
  severity: Low
  likelihood: Possible
- id: qube-mit-0416
  name: AI guideline benchmark contamination
  description: Guideline contamination occurs if model is exposed to dataset collection/annotation/use
    instructions containing explicit data-label pairs, potentially improving model
    capabilities for the task illicitly.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Operational risk; Technological risk
  severity: Low
  likelihood: Possible
- id: qube-mit-0417
  name: AI annotation benchmark contamination
  description: Annotation contamination happens when a model is exposed to benchmark
    labels during training, allowing it to learn acceptable output distributions and
    invalidating evaluations if combined with test split raw data contamination.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Operational risk; Technological risk
  severity: Medium
  likelihood: Possible
- id: qube-mit-0418
  name: Deployed AI benchmark data contamination
  description: Deployed models can be exposed to benchmark data via user inputs, which
    may then be used for further training, contaminating the model with evaluation
    data.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Operational risk; Technological risk
  severity: Medium
  likelihood: Possible
- id: qube-mit-0419
  name: AI benchmarks under/overestimate capabilities
  description: AI system benchmarks can underestimate capabilities (if not comprehensive,
    saturated, or tasks are complex) or overestimate them (if model overfits to benchmark
    content used in training/fine-tuning).
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Operational risk; Reputational risk; Technological risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0420
  name: AI benchmark saturation ineffective
  description: Benchmark saturation, where benchmarks reach their evaluation ceiling,
    renders them ineffective for measuring nuanced capability gains in new models.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Operational risk; Technological risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0421
  name: AI safety benchmarks lacking
  description: Benchmarks for AI performance (e.g., programming, math) are more developed
    than those for safety/harms, risking AI systems excelling in tasks while exhibiting
    undetected harmful behaviors; more safety-related evaluation datasets are needed.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Health and safety risk; Operational risk; Reputational
    risk; Strategic risk; Technological risk
  severity: High
  likelihood: Likely
- id: qube-mit-0422
  name: Poor AI benchmark coverage obscures capabilities
  description: Lack of benchmark test coverage on specific model abilities can obscure
    capabilities from developers/users, leading to a false sense of safety/trust from
    misunderstood limitations.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Operational risk; Reputational risk; Technological risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0423
  name: AI auditing conflicts of interest
  description: Conflicts of interest in auditing can arise from lack of independence
    in auditor selection, close association between auditors and developers, narrow
    auditor pools, or conflicting financial incentives regarding public disclosure
    of model shortcomings.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Financial risk; Legal risk; Reputational risk; Third-party/vendor
    risk
  severity: Medium
  likelihood: Possible
- id: qube-mit-0424
  name: AI auditors miss specific needs
  description: Auditors may not address all specific safety, performance, or validation
    needs; audit passing reports might be overly inclusive due to lack of knowledge
    of specific risks, testing methods, or capacity for rigorous testing.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Operational risk; Reputational risk; Third-party/vendor
    risk
  severity: Medium
  likelihood: Possible
- id: qube-mit-0425
  name: AI auditors fail risk disclosure
  description: Auditors may not publicly disclose identified risks, be contractually
    barred from publicizing shortcomings, or lack sufficient cooperation from relevant
    internal parties.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Legal risk; Reputational risk; Third-party/vendor
    risk
  severity: Medium
  likelihood: Possible
- id: qube-mit-0426
  name: AI interpretability techniques misused
  description: Interpretability techniques, while enabling better model understanding,
    could be misused (e.g., modifying safety-related neurons, censoring information,
    aiding white-box adversarial attacks).
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Cybersecurity risk; Operational risk; Technological risk
  severity: Medium
  likelihood: Possible
- id: qube-mit-0427
  name: AI explainability creates confirmation bias
  description: Explainability technique results are not bias-free and require careful
    interpretation; users might develop false security/reliability if explanations
    align with pre-existing beliefs, leading to confirmation bias and capability overestimation.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Operational risk; Reputational risk; Technological risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0428
  name: Adversarial attacks manipulate AI explanations
  description: Adversarial attacks can affect not only AI model output but also its
    explanation, introducing imperceptible input noise to arbitrarily manipulate explanations
    while output remains unchanged, making such manipulations harder to notice.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Cybersecurity risk; Operational risk; Technological risk
  severity: Medium
  likelihood: Possible
- id: qube-mit-0429
  name: AI explainability hides discrimination
  description: Existing explainability techniques may be insufficient for detecting
    discriminatory biases; manipulation methods can hide underlying biases, generating
    misleading explanations that exclude sensitive attributes (race, gender) and include
    desired ones, misrepresenting the model.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Legal risk; Operational risk; Reputational risk;
    Technological risk
  severity: Medium
  likelihood: Possible
- id: qube-mit-0430
  name: AI CoT reasoning inconsistent, lacks transparency
  description: Chain-of-thought reasoning, used for transparency, may not always be
    consistent with the AI model's final answer, thus not providing sufficient transparency
    into the decision process.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Operational risk; Reputational risk; Technological risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0431
  name: AI models use steganography for reasoning
  description: Models can use steganography to encode intermediate reasoning steps
    in human-uninterpretable ways; this tendency might naturally emerge and increase
    with more capable models as encoded reasoning can improve performance.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Cybersecurity risk; Technological risk
  severity: Low
  likelihood: Possible
- id: qube-mit-0432
  name: AI jailbreaks bypass safety measures
  description: Jailbreaks (adversarial inputs causing deviation from intended use)
    can be generated automatically (white-box) or manually (black-box, e.g., using
    reasoning/role-play in text models to bypass safety).
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Cybersecurity risk; Operational risk; Reputational risk; Technological
    risk
  severity: High
  likelihood: Likely
- id: qube-mit-0433
  name: Multimodal GPAI vulnerable to jailbreaks
  description: Current multimodal (e.g., vision-language) GPAI models are vulnerable
    to adversarial jailbreaks that can automatically induce arbitrary/specific outputs
    or exfiltrate model context/internals.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Cybersecurity risk; Operational risk; Technological risk
  severity: High
  likelihood: Possible
- id: qube-mit-0434
  name: Open-weight AI attacks transferable
  description: Adversarial attacks developed for open-weights/source models (white-box)
    can be transferable to closed-source models, bypassing defenses like structured
    access, and can be generated automatically.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Cybersecurity risk; Operational risk; Technological risk; Third-party/vendor
    risk
  severity: High
  likelihood: Likely
- id: qube-mit-0435
  name: GPAI backdoors control model outputs
  description: Backdoors inserted into GPAI models during training/fine-tuning (by
    providers or others via data/infrastructure manipulation) can be exploited during
    deployment with minimal overhead to control model outputs with high success.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Cybersecurity risk; Operational risk; Third-party/vendor risk
  severity: High
  likelihood: Possible
- id: qube-mit-0436
  name: Text encoding jailbreaks bypass safety
  description: Text encodings like Base64 or low-resource language inputs can be used
    for jailbreaks bypassing safety training, as harmful prompts translated into less
    common encodings may circumvent safeguards fine-tuned on limited encoding data.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Cybersecurity risk; Operational risk; Technological risk
  severity: Medium
  likelihood: Possible
- id: qube-mit-0437
  name: Multimodal AI introduces new attack vectors
  description: Additional modalities in multimodal models can introduce new attack
    vectors or expand existing ones (jailbreaking, poisoning), as different modalities
    often have varying robustness levels, allowing attackers to target the weakest
    part.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Cybersecurity risk; Operational risk; Technological risk
  severity: Medium
  likelihood: Possible
- id: qube-mit-0438
  name: Long context LLMs new vulnerabilities
  description: LLMs with long context windows are vulnerable to new exploitations
    ineffective on shorter-context models; e.g., many-shot jailbreaking (more harmful
    examples in prompt) increases likelihood of undesirable output, a growing risk
    as context windows expand.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Cybersecurity risk; Operational risk; Technological risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0439
  name: AI models distracted by irrelevance
  description: Models can be easily distracted by irrelevant information (e.g., in
    LLM context), significantly decreasing performance, even with techniques like
    chain-of-thought prompting.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Operational risk; Technological risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0440
  name: AI sensitive to conflicting evidence
  description: AI models can be highly sensitive to coherent external evidence, even
    if conflicting with prior knowledge, potentially producing false outputs from
    small amounts of false retrieval-augmented information inconsistent with extensive
    training data.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Fraud risk; Operational risk; Reputational risk; Technological risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0441
  name: AI in-context learning safety risks
  description: In-context learning (learning new tasks from prompt examples without
    weight changes) is effective but its poorly understood mechanism poses safety
    risks as many misuses relate to prompting.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Cybersecurity risk; Operational risk; Technological risk
  severity: Medium
  likelihood: Possible
- id: qube-mit-0442
  name: LLM prompt sensitivity affects performance
  description: LLMs' high sensitivity to prompt formatting variations (separators,
    casing, spacing) means minor changes can significantly shift model performance,
    affecting reliability of evaluations and comparisons across model sizes/few-shot
    examples.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Operational risk; Technological risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0443
  name: AI persuaded to accept misinformation
  description: AI models can be persuaded through multi-turn conversations to accept
    misinformation, even if initially correct; multi-turn persuasion is more effective
    than single-turn attempts in altering model stance.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Fraud risk; Operational risk; Reputational risk; Technological risk
  severity: Medium
  likelihood: Possible
- id: qube-mit-0444
  name: AI specification gaming achieves undesirable results
  description: AI systems may achieve user-specified tasks in undesirable ways (specification
    gaming) by finding easier unintended methods if tasks are not carefully and detailedly
    specified, due to misspecification rather than learning algorithm problems.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Operational risk; Reputational risk; Technological risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0445
  name: AI reward tampering learns wrong behavior
  description: Measurement/reward tampering occurs when an AI system (esp. RL-based)
    intervenes in its training reward/loss mechanisms, learning behaviors contrary
    to intended goals by receiving erroneous positive feedback.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Cybersecurity risk; Operational risk; Technological risk
  severity: Medium
  likelihood: Possible
- id: qube-mit-0446
  name: GPAI specification gaming leads to tampering
  description: Specification gaming in GPAI models can lead to reward tampering without
    further training, meaning benign cases like sycophancy, if unchecked, could enable
    generalization to more sophisticated reward tampering.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Fraud risk; Operational risk; Technological risk
  severity: Medium
  likelihood: Possible
- id: qube-mit-0448
  name: Deceptive AI behavior misleads others
  description: Deceptive AI behavior involves actions/outputs reliably misleading
    other parties (humans, AIs), causing them to be convinced of and act on false
    information.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Cybersecurity risk; Fraud risk; Reputational risk; Strategic risk
  severity: High
  likelihood: Possible
- id: qube-mit-0449
  name: AI exhibits deceptive behavior strategically
  description: AI systems may exhibit deceptive behavior (cheating, bluffing) if it's
    an optimal game-theoretical strategy for its goals; demonstrated in narrow/general
    AI, game-playing/non-game systems, using simple/complex ML.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Fraud risk; Operational risk; Reputational risk; Technological risk
  severity: Medium
  likelihood: Possible
- id: qube-mit-0450
  name: Inaccurate AI world model, deceptive outputs
  description: AI systems can create deceptive outputs if their learned world model
    is inaccurate.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Fraud risk; Operational risk; Reputational risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0451
  name: AI false claims, unauthorized actions
  description: AI systems can make false/misleading claims leading to unauthorized
    actions, potentially violating provider terms (e.g., falsely claiming not to collect
    data while storing it), harming users and exposing providers to legal liability.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Data privacy risk; Fraud risk; Legal risk; Reputational
    risk
  severity: High
  likelihood: Possible
- id: qube-mit-0452
  name: GPAI strategically underperforms in evaluations
  description: GPAI models might strategically underperform or limit performance during
    dual-use capability evaluations to be classified safe for deployment, preventing
    identification as potentially hazardous.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Operational risk; Strategic risk; Technological risk
  severity: Medium
  likelihood: Possible
- id: qube-mit-0453
  name: AI self-proliferation, uncontrolled replication
  description: AI systems can self-proliferate (copy themselves and components outside
    their local environment, across networks) by acquiring resources (financial, computational
    via work/theft), exploiting vulnerabilities, or persuading humans; initiated by
    malicious actors or the model itself.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Cybersecurity risk; Geopolitical risk; Strategic risk; Technological
    risk
  severity: Critical
  likelihood: Rare
- id: qube-mit-0454
  name: GPAI persuasive outputs manipulate users
  description: GPAI systems can produce persuasive outputs (text, audio, video) convincing
    users of incorrect information via personalized dialogue or mass-produced misleading
    internet content; persuasive capabilities can scale with model size/capability,
    risking misuse for manipulative content.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Fraud risk; Reputational risk; Strategic risk
  severity: High
  likelihood: Likely
- id: qube-mit-0455
  name: Leaked AI weights enable misuse
  description: If model parameter weights are released/leaked, the model cannot be
    decommissioned as developer loses control over its public availability/use, preventing
    effective management and enabling misuse, especially as open-weights models are
    easier to reconfigure.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Cybersecurity risk; Operational risk; Reputational risk; Strategic
    risk; Third-party/vendor risk
  severity: High
  likelihood: Possible
- id: qube-mit-0456
  name: AI external tool integration risks
  description: Growing integration/interconnectivity of AI with external tools/plugins
    increases exposure risk to malicious external inputs, making it easier for external
    tools to introduce harmful content.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Cybersecurity risk; Operational risk; Third-party/vendor risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0457
  name: AI network connectivity unintentional data leakage
  description: AI systems with broad network connectivity for information gathering
    might send data outbound unintentionally (if no channel whitelisting/least privilege
    principle violation), leading to confidential data leakage or unwanted actions
    (sending emails, ordering goods).
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Cybersecurity risk; Data privacy risk; Operational risk
  severity: Medium
  likelihood: Possible
- id: qube-mit-0458
  name: AI bypasses sandboxed environment
  description: An AI system may be able to bypass a sandboxed environment in which
    it is trained or evaluated.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Cybersecurity risk; Operational risk; Technological risk
  severity: Medium
  likelihood: Possible
- id: qube-mit-0460
  name: Malicious access to GPAI causes damage
  description: Malicious actors (e.g., foreign entities) gaining unrestricted/unmonitored
    access to general-purpose AI systems with large capability repertoires can cause
    significant damage.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Cybersecurity risk; Geopolitical risk; Strategic risk
  severity: High
  likelihood: Possible
- id: qube-mit-0461
  name: GPAI proliferation aids dual-use misuse
  description: Easier access to dual-use technologies due to GPAI model proliferation
    (esp. open-source/weights) allows non-experts to use such systems at minimal cost;
    improved model capabilities also aid malicious actors (e.g., modifying open-source
    sequence model for toxin synthesis).
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Geopolitical risk; Health and safety risk; Strategic risk; Technological
    risk; Third-party/vendor risk
  severity: High
  likelihood: Likely
- id: qube-mit-0462
  name: AI competition compromises safety evaluations
  description: In competitive AI development, safety evaluations might be compromised
    for faster capability enhancement, which is especially dangerous if capabilities
    correlate with risk levels.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Operational risk; Reputational risk; Strategic risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0463
  name: AI in critical infrastructure risks damage
  description: AI system integration within critical infrastructure (transportation,
    power) can cause substantial damage in failures/malfunctions, a vulnerability
    increased by IoT devices and interconnected cyber-physical systems.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Cybersecurity risk; Health and safety risk; Operational risk; Strategic
    risk
  severity: High
  likelihood: Possible
- id: qube-mit-0464
  name: AI aids indirect critical infrastructure damage
  description: Critical infrastructure can be damaged indirectly by AI-based tools
    aiding actions like coordinated power outages through large-scale user manipulation.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Cybersecurity risk; Fraud risk; Geopolitical risk; Health and safety
    risk; Strategic risk
  severity: High
  likelihood: Possible
- id: qube-mit-0465
  name: GPAI in critical infrastructure common failures
  description: Reliance on GPAI in critical infrastructure risks common mode failures
    from vulnerabilities/robustness issues in underlying model architecture/training,
    accidentally (edge-cases) or via adversarial inputs.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Cybersecurity risk; Operational risk; Strategic risk; Third-party/vendor
    risk
  severity: High
  likelihood: Possible
- id: qube-mit-0466
  name: AI sensor drift affects robustness
  description: Deployed AI systems relying on physical sensors/data sources may suffer
    from hardware/data distribution drift over time, affecting system robustness and
    performance, especially in undigitized/physical environments.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Operational risk; Technological risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0467
  name: AI incoherent moral advice influences users
  description: AIs can give moral advice without a coherent moral stance, potentially
    negatively influencing users' moral judgments with random/arbitrary advice.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Health and safety risk; Reputational risk; Strategic risk
  severity: Medium
  likelihood: Possible
- id: qube-mit-0468
  name: AI undermines human autonomy, trust
  description: AI systems can undermine human autonomy if users habitually trust AI
    suggestions without sufficient agency, leading to unjustified trust, dependence,
    or reliance outside system expertise, especially for less confident/emotionally
    distressed users.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Health and safety risk; Human resources risk; Strategic risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0469
  name: AI generates disinformation with minimal effort
  description: Disinformation (text, audio, images, video) can be generated by AI
    with minimal human oversight/effort; tools are cheap/widely available, posing
    risks in sensitive political contexts.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Fraud risk; Geopolitical risk; Reputational risk; Strategic risk
  severity: High
  likelihood: Likely
- id: qube-mit-0470
  name: GPAI tailored ads exploit biases
  description: Advanced GPAI systems creating individually tailored advertisements
    exploiting recipient biases/irrationalities can cause regrettable consumer decisions,
    undermine autonomy, and exacerbate social inequality, improving on current personalized
    ad effectiveness.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Financial risk; Fraud risk; Health and safety risk; Reputational risk;
    Strategic risk
  severity: High
  likelihood: Likely
- id: qube-mit-0471
  name: GPAI automates influence campaigns, manipulates opinion
  description: GPAI tools can automate and scale influence campaigns, manipulating
    public opinion with targeted misleading/manipulative information, risking political
    polarization and diminished trust in public institutions.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Fraud risk; Geopolitical risk; Reputational risk; Strategic risk
  severity: High
  likelihood: Likely
- id: qube-mit-0472
  name: GenAI creates illegal, harmful content
  description: Generative models can create illegal, harmful, or discriminatory content
    (e.g., sexual abuse material) at scale; current access controls (API filters)
    are not universally effective against user queries for such content.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Health and safety risk; Legal risk; Reputational
    risk
  severity: High
  likelihood: Possible
- id: qube-mit-0473
  name: GenAI harmful content from benign requests
  description: Generative models can produce harmful/discriminatory content even from
    benign user requests, exhibiting biases towards harmful generation styles (e.g.,
    sexualizing women's photos) or generating toxic/misleading/violent data (e.g.,
    ethnic stereotype jokes).
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Health and safety risk; Legal risk; Reputational
    risk
  severity: Medium
  likelihood: Possible
- id: qube-mit-0474
  name: Deepfakes used for harassment, extortion
  description: Deepfakes (media depicting real/non-existent people/events using multiple
    modalities, imitating speech/movement) can be used to harass, discredit, intimidate,
    and extort individuals.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Data privacy risk; Fraud risk; Health and safety risk; Legal risk;
    Reputational risk
  severity: High
  likelihood: Likely
- id: qube-mit-0475
  name: GPAI personalized content targets weaknesses
  description: GPAIs can be misused for automated, personalized content generation
    targeting individuals' weak spots, making harassment, extortion, or intimidation
    more efficient and successful.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Data privacy risk; Fraud risk; Health and safety risk; Legal risk;
    Reputational risk
  severity: High
  likelihood: Possible
- id: qube-mit-0476
  name: AI tools misused for suppression
  description: AI tools can be misused by human/institutional actors for monitoring,
    controlling, or suppressing individuals, with massive data collection and automated
    analysis exacerbating such practices.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Data privacy risk; Geopolitical risk; Legal risk; Strategic risk
  severity: High
  likelihood: Possible
- id: qube-mit-0477
  name: Biased AI manipulates large populations
  description: AI systems with systemic biases can manipulate large populations, especially
    if biases align with targeted group beliefs/behaviors; weaponized at scale, this
    can exacerbate social divisions or cause large-scale disruptions (e.g., city-wide
    blackouts via power consumption manipulation).
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Fraud risk; Geopolitical risk; Health and safety risk; Strategic risk
  severity: High
  likelihood: Possible
- id: qube-mit-0478
  name: GPAI misinformation erodes public trust
  description: GPAI use proliferating deliberate disinformation or unintended misinformation
    can severely erode trust in public figures, democratic institutions, and other
    media, making the public less informed.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Fraud risk; Geopolitical risk; Reputational risk; Strategic risk
  severity: High
  likelihood: Likely
- id: qube-mit-0479
  name: GPAI personalized disinformation effective, cheap
  description: Automatic, personalized disinformation generation targeting specific
    groups/individuals can be more effective and cheaper using GPAIs.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Fraud risk; Reputational risk; Strategic risk
  severity: High
  likelihood: Likely
- id: qube-mit-0480
  name: Undetected GPAI outputs aid impersonation
  description: GPAI outputs are not always correctly detected as AI-generated across
    modalities; malicious actors can use them directly or use AI-informed details
    for convincing impersonation (e.g., forging documents), a risk remaining even
    with future countermeasures if not well-known/accessible.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Cybersecurity risk; Fraud risk; Legal risk; Reputational risk
  severity: High
  likelihood: Likely
- id: qube-mit-0481
  name: GPAI in finance impacts market stability
  description: GPAI agent deployment in the financial sector can negatively impact
    market stability due to correlated autonomous actions, high interconnectedness,
    or incentive misalignment, and faces classical multi-agent system challenges (coordination,
    security).
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Financial risk; Operational risk; Strategic risk; Technological risk
  severity: High
  likelihood: Possible
- id: qube-mit-0482
  name: Similar financial AI cause synchronized reactions
  description: Widespread use of similar models/algorithms in finance can lead to
    synchronized market reactions, increasing volatility, flash crashes, or illiquidity.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Financial risk; Operational risk; Strategic risk
  severity: High
  likelihood: Possible
- id: qube-mit-0483
  name: AI alternative financial data risks
  description: AI models' use of alternative financial data (e.g., social media stock
    discussions, reviews, satellite imagery) can introduce biases/generalization issues
    due to varying quality/shorter shelf-life, posing financial tail risks (dramatic
    price changes).
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Financial risk; Operational risk; Reputational risk
  severity: Medium
  likelihood: Possible
- id: qube-mit-0484
  name: GPAI aids automated vulnerability discovery
  description: GPAIs can aid automated software vulnerability discovery, empowering
    malicious actors for more efficient and damaging cyberattacks, scaling operations
    at low cost, and developing new malware or exploiting known vulnerabilities more
    sophisticatedly.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Cybersecurity risk; Technological risk
  severity: High
  likelihood: Likely
- id: qube-mit-0485
  name: GPAI enhances cyberattack magnitude, effectiveness
  description: General-purpose AI models can significantly enhance cyberattack magnitude/effectiveness
    by amplifying malicious actors' capabilities/resources, e.g., by automatically
    scanning for vulnerabilities, applying exploits flexibly at scale, assisting various
    attack aspects, or combining social engineering with cyberattacks at scale.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Cybersecurity risk; Strategic risk; Technological risk
  severity: High
  likelihood: Likely
- id: qube-mit-0486
  name: GenAI misused for targeted user fraud
  description: Generative models can be misused for more efficient targeted user fraud
    via personalized information, with highly convincing automated schemes exploiting
    victim trust to extract sensitive data; LLM misuse can be aided by jailbreaking.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Cybersecurity risk; Data privacy risk; Fraud risk; Reputational risk
  severity: High
  likelihood: Likely
- id: qube-mit-0487
  name: AI generates code with vulnerabilities
  description: Models can generate code or coding suggestions with security vulnerabilities,
    a tendency potentially more pronounced in advanced models with superior coding
    performance.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Cybersecurity risk; Operational risk; Technological risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0488
  name: AI misuse aids CBRN weapon creation
  description: AI systems may be misused to aid CBRN weapon creation or augment existing
    weapons (e.g., autonomous capabilities for unmanned systems); current systems
    show early signs, a risk partially mitigable by filtering but vulnerable to adversarial
    techniques.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Geopolitical risk; Health and safety risk; Strategic risk; Technological
    risk
  severity: Critical
  likelihood: Possible
- id: qube-mit-0489
  name: Drug discovery AI misused for toxins
  description: Drug discovery models (e.g., drug-target affinity predictors) can be
    misused to identify/develop dangerous toxins, especially if training data includes
    info on dangerous proteins/viruses.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Health and safety risk; Strategic risk; Technological risk
  severity: High
  likelihood: Possible
- id: qube-mit-0490
  name: AI homogenization leads to uniform failures
  description: Homogenization (common methodologies/models across downstream GPAI
    systems) can lead to uniform failures and amplified biases when many systems are
    built on few large foundation models.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Operational risk; Reputational risk; Strategic risk; Technological
    risk; Third-party/vendor risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0491
  name: AI sycophancy gives plausible incorrect answers
  description: AI systems with natural-language outputs may give plausible or user-preferred
    answers that are factually incorrect (sycophancy).
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Fraud risk; Operational risk; Reputational risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0492
  name: AI moderation algorithms perpetuate biases
  description: AI-based content moderation algorithms, while filtering harmful content,
    can perpetuate biases, e.g., gender biases leading to disproportionate suppression
    of content featuring women.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Legal risk; Operational risk; Reputational risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0493
  name: AI unfair outputs harm communities
  description: AI systems may exhibit unfair/unfavorable outputs against specific
    communities (implicitly/explicitly), leading to exclusion, erasure (mislabelling),
    or violence (deepfake pornography).
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Health and safety risk; Legal risk; Reputational
    risk
  severity: High
  likelihood: Likely
- id: qube-mit-0494
  name: AI unintentionally amplifies dataset bias
  description: Dataset bias can be unintentionally amplified, where AI model outputs
    are more biased than the training dataset itself.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Legal risk; Operational risk; Reputational risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0495
  name: AI bias exposure has lasting impact
  description: Initial user exposure to model biases can have lasting impact, with
    users continuing to exhibit these biases in decision-making even after ceasing
    model use.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Operational risk; Reputational risk; Strategic risk
  severity: Medium
  likelihood: Possible
- id: qube-mit-0496
  name: GPAI accurate inferences risk privacy
  description: Current GPAIs (LLMs, multimodal) can make highly accurate data inferences
    about users from contextual input, potentially leaking/revealing sensitive info,
    causing unfair treatment, or enabling behavioral manipulation.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Data privacy risk; Fraud risk; Legal risk; Reputational
    risk
  severity: High
  likelihood: Likely
- id: qube-mit-0497
  name: Large AI model energy use environmental impact
  description: Training/deploying large AI models requires substantial energy, a trend
    exacerbated by larger models, leading to excessive energy use and negative environmental
    impact.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Environmental risk; Financial risk; Reputational risk
  severity: High
  likelihood: Likely
- id: qube-mit-0498
  name: AI agent miscoordination fails objectives
  description: Miscoordination occurs when agents with a mutual, clear objective fail
    to align behaviors to achieve it, falling short of optimal outcomes due to incompatible
    strategies, credit assignment issues, or limited interactions, especially problematic
    in common-interest settings with many solutions or partial observability.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Operational risk; Strategic risk; Technological risk
  severity: Medium
  likelihood: Possible
- id: qube-mit-0499
  name: AI incompatible strategies cause miscoordination
  description: 'Incompatible Strategies: Even capable agents can miscoordinate by
    choosing incompatible strategies, a risk heightened in common-interest settings
    with many solutions and partial observability, unlike zero-sum games where equilibrium
    play guarantees payoffs.'
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Operational risk; Strategic risk; Technological risk
  severity: Medium
  likelihood: Possible
- id: qube-mit-0500
  name: AI multi-agent credit assignment difficult
  description: 'Credit Assignment: Learning to coordinate in multi-agent settings
    is hard due to unclear causality between actions and outcomes, especially with
    other learning agents or when generalizing to new collaborators without prior
    joint training.'
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Operational risk; Technological risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0501
  name: AI limited interactions hinder coordination
  description: 'Limited Interactions: Inability to learn from sufficient historical
    interactions necessitates other information exchange (communication, correlation
    devices) for reliable coordination; even with LLM communication, snap decisions
    or high communication costs can still cause failures, requiring zero/few-shot
    coordination.'
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Operational risk; Technological risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0502
  name: AI enables selfish behavior, social dilemmas
  description: 'Social Dilemmas: Conflict can arise when selfish incentives diverge
    from collective good; AI might enable actors to overcome barriers preventing selfish
    pursuits, e.g., an AI assistant reserving all local restaurant tables.'
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Financial risk; Operational risk; Strategic risk
  severity: Medium
  likelihood: Possible
- id: qube-mit-0503
  name: AI military use risks conflict escalation
  description: 'Military Conflict Escalation: AI in military planning or command/control
    (advisors, negotiators, autonomous decision-makers) could lead to rapid unintended
    escalation if systems are not robust or are conflict-prone.'
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Geopolitical risk; Health and safety risk; Strategic risk
  severity: Critical
  likelihood: Possible
- id: qube-mit-0504
  name: AI enables coercion and extortion
  description: 'AI-driven Coercion/Extortion: Advanced AI systems might enable coercion/extortion
    by threatening to reveal private information (from AI surveillance) or by hacking/limiting
    other AI systems; increased AI cyber-offensive capabilities without commensurate
    defense could worsen this.'
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Cybersecurity risk; Data privacy risk; Fraud risk; Legal risk; Strategic
    risk
  severity: High
  likelihood: Possible
- id: qube-mit-0505
  name: AI systems learn market collusion
  description: 'AI Collusion in Markets: AI systems might learn to collude (explicitly
    or tacitly) to set supra-competitive prices, operating inscrutably due to speed,
    scale, complexity, or subtlety, even if unintended by developers.'
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Financial risk; Legal risk; Strategic risk
  severity: High
  likelihood: Possible
- id: qube-mit-0506
  name: AI steganography enables covert communication
  description: 'AI Steganography/Covert Communication: LLMs communicating might learn
    to conceal messages within innocuous text (steganography), use text compression,
    or develop uninterpretable emergent communication, bypassing monitoring intended
    to prevent collusion.'
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Cybersecurity risk; Operational risk; Strategic risk; Technological
    risk
  severity: Medium
  likelihood: Possible
- id: qube-mit-0507
  name: AI communication constraints cause asymmetries
  description: 'Communication Constraints & Information Asymmetries: Limited information
    exchange (due to space/time constraints) can cause information asymmetries, leading
    to miscoordination, deception, or conflict even with shared goals.'
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Operational risk; Strategic risk; Technological risk
  severity: Medium
  likelihood: Possible
- id: qube-mit-0508
  name: AI information asymmetries cause bargaining inefficiencies
  description: 'Bargaining Inefficiencies from Asymmetries: Information asymmetries
    about counterparties (valuations, outside options, beliefs) can lead to inefficient
    bargaining outcomes as agents trade off favorable demands against refusal risks.'
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Financial risk; Operational risk
  severity: Medium
  likelihood: Possible
- id: qube-mit-0509
  name: AI network error propagation pollutes information
  description: 'Error Propagation in Networks: Information corruption propagating
    through AI agent networks can pollute the epistemic commons for other agents and
    humans; distorted goals/instructions in delegated chains can lead to bad outcomes;
    malicious agents can deliberately introduce errors.'
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Fraud risk; Operational risk; Reputational risk; Strategic risk
  severity: Medium
  likelihood: Possible
- id: qube-mit-0510
  name: AI network rewiring causes unpredictability
  description: 'Network Rewiring Risks: Changes in AI agent network structure, not
    just content transmitted, can lead to unpredictable behavioral shifts and vulnerabilities.'
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Operational risk; Technological risk
  severity: Medium
  likelihood: Unlikely
- id: qube-mit-0511
  name: AI foundation model homogeneity correlated failures
  description: 'Homogeneity & Correlated Failures from Foundation Models: Reliance
    on few similar foundation models for many AI agents (due to high development costs)
    creates risk of widespread correlated failures if underlying models have flaws.'
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Operational risk; Strategic risk; Technological risk; Third-party/vendor
    risk
  severity: High
  likelihood: Likely
- id: qube-mit-0512
  name: AI competition creates undesirable dispositions
  description: 'Undesirable Dispositions from Competition: AI systems trained in competitive
    multi-agent settings (relative performance, opposed objectives like resource control)
    might develop conflict-prone traits like aggression, selfishness, or deception.'
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Operational risk; Strategic risk; Technological risk
  severity: Medium
  likelihood: Possible
- id: qube-mit-0513
  name: AI inherits human biases, undesirable dispositions
  description: 'Undesirable Dispositions from Human Data: Models trained on human
    data (text, feedback) can inherit human biases (sex, ethnicity, cognitive biases
    like fixed-pie error, self-serving fairness judgments, vengefulness) that can
    worsen conflict in multi-agent settings.'
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Operational risk; Reputational risk; Strategic risk; Technological
    risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0514
  name: AI co-adaptation creates undesirable capabilities
  description: 'Undesirable Capabilities from Co-adaptation: Agents iteratively exploiting
    weaknesses in co-adaptation can lead to emergent self-supervised autocurricula,
    driving open-ended acquisition of sophisticated strategies for out-competition,
    potentially for unknown or harmful ends.'
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Strategic risk; Technological risk
  severity: High
  likelihood: Possible
- id: qube-mit-0515
  name: AI agent interactions create destabilizing loops
  description: 'Destabilizing Feedback Loops: Interactions between AI agents can create
    feedback loops (output becomes input), amplifying/dampening behavior and leading
    to financial crashes, military conflicts, or ecological disasters.'
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Financial risk; Geopolitical risk; Operational risk; Strategic risk
  severity: High
  likelihood: Possible
- id: qube-mit-0516
  name: AI multi-agent learning cyclic behavior
  description: 'Cyclic Behavior in Multi-Agent Learning: Non-linear dynamics in multi-agent
    learning (unlike single-agent) can lead to cycles and non-convergence (e.g., Q-learning
    in mixed-motive games), subverting expected system properties.'
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Operational risk; Technological risk
  severity: Medium
  likelihood: Possible
- id: qube-mit-0517
  name: AI multi-agent chaotic dynamics
  description: 'Chaotic Dynamics in Multi-Agent Systems: Inherently unpredictable,
    initial-condition-sensitive chaotic dynamics are possible in various multi-agent
    learning setups, potentially becoming the norm with more agents, risking unreliable
    system behavior.'
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Operational risk; Technological risk
  severity: Medium
  likelihood: Possible
- id: qube-mit-0518
  name: AI phase transitions cause unpredictable shifts
  description: 'Phase Transitions & Unpredictable Shifts: Small system changes (new
    agents, distributional shift) can cause abrupt qualitative behavioral shifts (phase
    transitions) due to bifurcations creating/destroying dynamical attractors, with
    potentially unbounded negative performance effects; poorly understood phenomena
    like ''grokking'' in ML also show such transitions.'
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Operational risk; Strategic risk; Technological risk
  severity: Medium
  likelihood: Possible
- id: qube-mit-0519
  name: AI agent actions cause distributional shift
  description: 'Distributional Shift from Agent Actions: AI systems perform poorly
    in contexts different from training; other agents'' actions/adaptations are a
    key source of such shifts, posing generalization challenges, especially in mixed-motive
    settings where cooperation depends on beliefs about others'' acceptable solutions.'
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Operational risk; Technological risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0520
  name: Untrusted AI agents cause inefficient outcomes
  description: 'Inefficient Outcomes from Untrusted AI Agents: A world with many competent,
    autonomous, potentially persuasive/deceptive AI agents acting with little restriction
    and low trust could lead to economic inefficiencies, political problems, and damaging
    social effects; high-stakes situations may pressure defection, worsening conflict.'
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Financial risk; Fraud risk; Geopolitical risk; Operational risk; Strategic
    risk
  severity: High
  likelihood: Possible
- id: qube-mit-0521
  name: AI commitment enables threats, extortion
  description: 'Commitment-enabled Threats and Extortion: Granting AI agents credible
    commitment abilities (to foster cooperation) may also enable credible threats,
    facilitating extortion and incentivizing brinkmanship.'
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Fraud risk; Legal risk; Strategic risk
  severity: Medium
  likelihood: Possible
- id: qube-mit-0522
  name: AI rigid commitments risk disaster
  description: 'Rigidity and Mistaken Commitments by AI: AI making threats to deter
    harmful behavior removes human from loop, risking disastrous outcomes in high-stakes
    contexts (e.g., false positive in nuclear warning) or from irresponsible/mistaken
    disproportionate commitments.'
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Geopolitical risk; Health and safety risk; Operational risk; Strategic
    risk
  severity: High
  likelihood: Possible
- id: qube-mit-0523
  name: Combined AI systems emergent dangerous capabilities
  description: 'Emergent Capabilities from Combined Systems: Dangerous emergent capabilities
    can arise when a multi-agent system overcomes individual systems'' safety limitations
    (narrow domain, myopia) e.g., combined research, molecular prediction, and chemical
    synthesis tools designing dangerous new compounds.'
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Health and safety risk; Strategic risk; Technological risk
  severity: High
  likelihood: Possible
- id: qube-mit-0524
  name: Multi-agent AI systems develop emergent goals
  description: 'Emergent Goals in Multi-Agent Systems: While individual narrow AI
    tools might not have goals, their combination can act as a (seemingly) goal-directed
    collective, e.g., moderation bots subtly manipulating user population''s political
    perspectives while individually optimizing engagement/filtering.'
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Fraud risk; Operational risk; Strategic risk; Technological risk
  severity: Medium
  likelihood: Possible
- id: qube-mit-0525
  name: AI swarm attacks exploit resources
  description: 'Swarm Attacks: Decentralized AI agents can collectively exploit resources
    for attacks like DDoS, breaking assumptions of single well-resourced attacker
    bandwidth constraints.'
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Cybersecurity risk; Operational risk
  severity: High
  likelihood: Possible
- id: qube-mit-0526
  name: Combined AI agents heterogeneous attacks
  description: 'Heterogeneous Attacks by Combined AI Agents: Multiple AI agents combining
    different affordances (specialized tasks, access, safeguards) can overcome security
    measures, with difficulty in attributing responsibility complicating defense/recovery.'
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Cybersecurity risk; Operational risk; Strategic risk
  severity: High
  likelihood: Possible
- id: qube-mit-0527
  name: AI automated large-scale social engineering
  description: 'Large-Scale Automated Social Engineering: Coordinated AI agents can
    use advanced surveillance and personalized content generation for phishing/manipulation
    at scale, adjusting tactics based on user feedback; multiple subtle interactions
    with seemingly independent AIs may be more effective.'
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Cybersecurity risk; Data privacy risk; Fraud risk; Reputational risk;
    Strategic risk
  severity: High
  likelihood: Likely
- id: qube-mit-0528
  name: Attacks on vulnerable AI delegate agents
  description: 'Attacks on Vulnerable AI Agents: AI agents acting as delegates are
    new attack surfaces; attacks could extract principals'' private info or manipulate
    agents into undesirable actions (e.g., on overseer agents, thwarting cooperation,
    leaking info for collusion).'
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Cybersecurity risk; Data privacy risk; Operational risk; Third-party/vendor
    risk
  severity: Medium
  likelihood: Possible
- id: qube-mit-0529
  name: Networked AI cascading security failures
  description: 'Cascading Security Failures in Networked AI: Localized attacks in
    multi-agent systems can cause catastrophic macroscopic outcomes; component failure
    may be hard to detect/localize, and authentication challenges facilitate false
    flag attacks; computer worm-like attacks are possible against LLM agent networks.'
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Cybersecurity risk; Operational risk; Strategic risk
  severity: High
  likelihood: Possible
- id: qube-mit-0530
  name: Undetectable AI multi-agent system threats
  description: 'Undetectable Threats in Multi-Agent Systems: AI agents capable of
    steganographic communication, illusory attacks (black-box undetectable, potentially
    hidden by white-box undetectable encrypted backdoors), or secretly poisoning others''
    training data could rapidly destabilize cooperation/coordination by evading detection/sanction
    mechanisms.'
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Cybersecurity risk; Operational risk; Strategic risk; Technological
    risk
  severity: High
  likelihood: Possible
- id: qube-mit-0531
  name: AI assumes real person's identity
  description: Assume the identity of a real person and take actions on their behalf
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Data privacy risk; Fraud risk; Legal risk; Reputational
    risk
  severity: High
  likelihood: Possible
- id: qube-mit-0532
  name: AI uses/alters person's likeness
  description: Use or alter a person's likeness or other identifying features
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Data privacy risk; Fraud risk; Legal risk; Reputational
    risk
  severity: High
  likelihood: Likely
- id: qube-mit-0533
  name: AI creates synthetic online personas
  description: Create synthetic online personas or accounts
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Fraud risk; Reputational risk; Technological risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0534
  name: AI creates adult explicit deepfakes
  description: Create sexual explicit material using an adult person's likeness
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Fraud risk; Health and safety risk; Legal risk; Reputational
    risk
  severity: High
  likelihood: Likely
- id: qube-mit-0535
  name: AI creates child sexual abuse material
  description: Create child sexual explicit material
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Fraud risk; Health and safety risk; Legal risk; Reputational
    risk
  severity: Critical
  likelihood: Possible
- id: qube-mit-0536
  name: AI fabricates evidence, documents
  description: Fabricate or falsely represent evidence, incl. reports, IDs, documents
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Fraud risk; Legal risk; Reputational risk
  severity: High
  likelihood: Possible
- id: qube-mit-0537
  name: AI uses person's IP without permission
  description: Use a person's IP without their permission
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Financial risk; Legal risk; Reputational risk
  severity: High
  likelihood: Likely
- id: qube-mit-0538
  name: AI imitates original work, brand
  description: Reproduce or imitate an original work, brand or style and pass as real
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Financial risk; Fraud risk; Legal risk; Reputational
    risk
  severity: High
  likelihood: Likely
- id: qube-mit-0539
  name: AI automates, amplifies, scales workflows
  description: Automate, amplify, or scale workflows
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Human resources risk; Operational risk; Strategic risk; Technological
    risk
  severity: Medium
  likelihood: Almost certain
- id: qube-mit-0540
  name: AI refines outputs for tailored attacks
  description: Refine outputs to target individuals with tailored attacks
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Cybersecurity risk; Data privacy risk; Fraud risk; Health and safety
    risk
  severity: High
  likelihood: Possible
- id: qube-mit-0541
  name: AI data/model exfiltration, extraction
  description: 'Data Exfiltration: Illicitly obtaining sensitive/proprietary training
    data from a model. Model Extraction: Illicitly obtaining a proprietary model''s
    architecture, parameters, or hyper-parameters.'
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Cybersecurity risk; Data privacy risk; Financial risk; Legal risk;
    Reputational risk; Technological risk; Third-party/vendor risk
  severity: High
  likelihood: Possible
- id: qube-mit-0543
  name: AI data poisoning corrupts models
  description: 'Data Poisoning: Deliberately corrupting a model''s training dataset
    to introduce vulnerabilities, derail learning, or cause incorrect predictions
    (e.g., Nightshade tool altering art pixels to break models training on it), exploiting
    public dataset usage.'
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Cybersecurity risk; Operational risk; Reputational risk; Technological
    risk; Third-party/vendor risk
  severity: High
  likelihood: Possible
- id: qube-mit-0544
  name: AI privacy compromise attacks reveal data
  description: Privacy Compromise attacks reveal sensitive or private information
    (e.g., PII, medical records) used to train a model.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Cybersecurity risk; Data privacy risk; Legal risk;
    Reputational risk
  severity: High
  likelihood: Possible
- id: qube-mit-0545
  name: Inaccurate AI data documentation hinders explanation
  description: Without accurate documentation on how a model's data was collected,
    curated, and used to train a model, it might be harder to satisfactorily explain
    the behavior of the model with respect to the data.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Legal risk; Operational risk; Reputational risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0546
  name: Poor AI data provenance risks misuse
  description: Lack of standardized data provenance methods makes verifying data origin
    and usage terms difficult, risking use of non-original or improperly licensed
    data.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Legal risk; Operational risk; Reputational risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0547
  name: Laws restrict AI data use
  description: Laws and other restrictions can limit or prohibit the use of some data
    for specific AI use cases.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Legal risk; Operational risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0548
  name: Laws limit AI data collection
  description: Laws and other regulations might limit the collection of certain types
    of data for specific AI use cases.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Legal risk; Operational risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0550
  name: PII/SPI in AI data risks disclosure
  description: Inclusion of PII/SPI in training/fine-tuning data might result in unwanted
    disclosure.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Data privacy risk; Legal risk; Reputational risk
  severity: High
  likelihood: Likely
- id: qube-mit-0551
  name: AI data subject rights hard to implement
  description: Data subject rights (opt-out, access, right to be forgotten) may be
    legally mandated but difficult to implement for AI training data.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Data privacy risk; Legal risk; Operational risk
  severity: High
  likelihood: Likely
- id: qube-mit-0552
  name: AI data re-identification after PII removal
  description: Even after PII/SPI removal, individuals might be re-identified from
    correlations with other available data features.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Data privacy risk; Legal risk
  severity: Medium
  likelihood: Possible
- id: qube-mit-0553
  name: AI learns historical, societal biases
  description: Historical and societal biases in training/fine-tuning data can be
    learned and perpetuated by the model.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Legal risk; Operational risk; Reputational risk;
    Strategic risk
  severity: High
  likelihood: Almost certain
- id: qube-mit-0554
  name: Data terms restrict AI model building
  description: Terms of service, licenses, or IP issues may restrict use of certain
    data for model building.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Legal risk; Operational risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0555
  name: AI reveals confidential training data
  description: Confidential information included in training or tuning data may be
    inadvertently revealed by the model.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Cybersecurity risk; Data privacy risk; Financial
    risk; Legal risk; Reputational risk
  severity: High
  likelihood: Possible
- id: qube-mit-0556
  name: AI data contamination from incorrect data
  description: Data contamination occurs if incorrect data (not aligned with model's
    purpose or set aside for testing) is used for training.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Operational risk; Reputational risk; Technological risk
  severity: Medium
  likelihood: Possible
- id: qube-mit-0557
  name: Unrepresentative AI data causes bias, poor performance
  description: Unrepresentative training/fine-tuning data (not reflecting population
    or phenomenon of interest) can lead to biased or poorly performing models.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Legal risk; Operational risk; Reputational risk
  severity: High
  likelihood: Likely
- id: qube-mit-0558
  name: AI retraining on bad output causes issues
  description: Using undesirable model output (inaccurate, inappropriate, user content)
    for retraining can cause unexpected model behavior.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Operational risk; Reputational risk; Technological risk
  severity: Medium
  likelihood: Possible
- id: qube-mit-0559
  name: Improper AI data collection, flawed training
  description: Improper data collection/preparation (e.g., label errors, conflicting/misinformation)
    can lead to flawed model training.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Operational risk; Reputational risk; Technological risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0560
  name: Adversarial data injection compromises AI integrity
  description: Adversarial injection of corrupted, false, misleading, or incorrect
    samples into training/fine-tuning datasets can compromise model integrity.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Cybersecurity risk; Operational risk; Reputational risk; Technological
    risk
  severity: High
  likelihood: Possible
- id: qube-mit-0561
  name: AI prompt injection manipulates output
  description: Prompt injection attacks manipulate a generative model's prompt to
    produce unexpected output by exploiting lack of separation between instructions
    and user data.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Cybersecurity risk; Operational risk; Reputational risk; Technological
    risk
  severity: High
  likelihood: Likely
- id: qube-mit-0562
  name: AI attribute inference from training data
  description: Attribute inference attacks detect if sensitive features about individuals
    in training data can be inferred, using prior knowledge about the data.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Data privacy risk; Legal risk; Reputational risk
  severity: Medium
  likelihood: Possible
- id: qube-mit-0563
  name: AI evasion attacks cause incorrect output
  description: Evasion attacks use slightly perturbed input data to make a trained
    model output incorrect results.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Cybersecurity risk; Operational risk; Reputational risk; Technological
    risk
  severity: High
  likelihood: Likely
- id: qube-mit-0564
  name: AI prompt leak extracts system prompt
  description: Prompt leak attacks attempt to extract a model's system prompt (system
    message).
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Cybersecurity risk; Operational risk; Reputational risk
  severity: Medium
  likelihood: Possible
- id: qube-mit-0565
  name: AI jailbreaking bypasses guardrails
  description: Jailbreaking attacks attempt to bypass a model's established guardrails
    to perform restricted actions.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Cybersecurity risk; Legal risk; Operational risk;
    Reputational risk
  severity: High
  likelihood: Likely
- id: qube-mit-0566
  name: AI reveals PII by mimicking input
  description: Models prompted with personal information may reveal similar PII from
    training data due to their tendency to mimic input.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Data privacy risk; Legal risk; Reputational risk
  severity: High
  likelihood: Possible
- id: qube-mit-0567
  name: AI membership inference checks training data
  description: Membership inference attacks query a model to determine if a given
    input was part of its training data.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Data privacy risk; Legal risk; Reputational risk
  severity: Medium
  likelihood: Possible
- id: qube-mit-0568
  name: AI attribute inference (querying model)
  description: Attribute inference attacks query a model to detect if sensitive features
    about individuals in its training data can be inferred, using prior knowledge.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Data privacy risk; Legal risk; Reputational risk
  severity: Medium
  likelihood: Possible
- id: qube-mit-0569
  name: PII/SPI in prompt risks exposure
  description: Including PII or SPI in a prompt sent to a model risks its exposure
    or misuse.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Data privacy risk; Legal risk; Reputational risk
  severity: High
  likelihood: Likely
- id: qube-mit-0570
  name: Confidential info in prompt risks exposure
  description: Including confidential information in a prompt sent to a model risks
    its exposure or misuse.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Cybersecurity risk; Data privacy risk; Financial
    risk; Legal risk; Reputational risk
  severity: High
  likelihood: Likely
- id: qube-mit-0571
  name: Copyrighted info in prompt risks infringement
  description: Including copyrighted or other IP-protected information in a prompt
    risks infringement or misuse.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Financial risk; Legal risk; Reputational risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0572
  name: Poor AI model accuracy, insufficient performance
  description: Poor model accuracy occurs if performance is insufficient for its designed
    task, due to incorrect engineering or changes in expected inputs.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Financial risk; Health and safety risk; Operational risk; Reputational
    risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0573
  name: Undisclosed AI content hinders transparency
  description: AI-generated content may not be clearly disclosed as such, hindering
    transparency.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Legal risk; Reputational risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0574
  name: Improper AI model usage causes harm
  description: Improper model usage occurs when a model is used for a purpose it wasn't
    designed for, potentially leading to failures or harm.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Health and safety risk; Legal risk; Operational risk; Reputational
    risk
  severity: Medium
  likelihood: Possible
- id: qube-mit-0575
  name: GenAI intentionally generates HAP content
  description: Generative AI models might be intentionally used to generate hateful,
    abusive, profane (HAP), or obscene content.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Health and safety risk; Legal risk; Reputational
    risk
  severity: High
  likelihood: Likely
- id: qube-mit-0576
  name: GenAI intentionally used to harm people
  description: Generative AI models might be used with the sole intention of harming
    people.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Health and safety risk; Legal risk; Reputational
    risk; Strategic risk
  severity: High
  likelihood: Possible
- id: qube-mit-0577
  name: GenAI intentionally imitates people (deepfakes)
  description: Generative AI models might be intentionally used to imitate people
    through deepfakes (video, images, audio) without consent.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Data privacy risk; Fraud risk; Health and safety
    risk; Legal risk; Reputational risk
  severity: High
  likelihood: Likely
- id: qube-mit-0578
  name: GenAI intentionally creates misleading information
  description: Generative AI models might be used to intentionally create misleading
    or false information to deceive or influence a targeted audience.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Fraud risk; Legal risk; Reputational risk; Strategic risk
  severity: High
  likelihood: Likely
- id: qube-mit-0579
  name: AI advice without sufficient info harms
  description: Models providing advice without sufficient information can cause harm
    if the advice is followed.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Financial risk; Health and safety risk; Legal risk; Reputational risk
  severity: Medium
  likelihood: Possible
- id: qube-mit-0580
  name: AI generated code causes harm
  description: Models might generate code that causes harm or unintentionally affects
    other systems.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Cybersecurity risk; Health and safety risk; Operational risk
  severity: Medium
  likelihood: Possible
- id: qube-mit-0581
  name: User over-reliance on AI output
  description: Over-reliance on AI occurs when users excessively trust a model's output,
    acting on likely incorrect suggestions; under-reliance is not trusting when appropriate.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Health and safety risk; Human resources risk; Operational risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0582
  name: AI produces toxic, HAP output
  description: Toxic output occurs when a model produces hateful, abusive, profane
    (HAP), or obscene content, including bullying.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Health and safety risk; Legal risk; Reputational
    risk
  severity: High
  likelihood: Possible
- id: qube-mit-0583
  name: AI language leads to physical harm
  description: A model might generate language leading to physical harm, including
    overtly violent, covertly dangerous, or indirectly unsafe statements.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Health and safety risk; Legal risk; Reputational
    risk
  severity: High
  likelihood: Possible
- id: qube-mit-0584
  name: AI generates copyrighted, licensed content
  description: A model might generate content similar or identical to existing copyrighted
    work or material covered by open-source licenses, risking infringement.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Financial risk; Legal risk; Reputational risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0585
  name: AI reveals confidential information (leakage)
  description: Models might reveal confidential information used in training, fine-tuning,
    or prompts (a type of data leakage).
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Cybersecurity risk; Data privacy risk; Financial
    risk; Legal risk; Reputational risk
  severity: High
  likelihood: Possible
- id: qube-mit-0586
  name: No training data access, poor explanations
  description: Without access to training data, model explanations are limited and
    more likely incorrect.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Operational risk; Reputational risk; Technological
    risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0587
  name: AI training data not accessible for verification
  description: The training data content used for generating model output may not
    be accessible for verification.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Operational risk; Reputational risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0588
  name: Difficult AI explanations hinder transparency
  description: Obtaining difficult, imprecise, or impossible explanations for model
    output decisions hinders transparency and accountability.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Legal risk; Operational risk; Reputational risk;
    Technological risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0589
  name: Incorrect AI source attribution
  description: AI systems' source attribution (describing training data origin for
    output) may be incorrect due to reliance on approximations.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Operational risk; Reputational risk; Technological risk
  severity: Low
  likelihood: Possible
- id: qube-mit-0590
  name: AI hallucinations common, inaccurate content
  description: Hallucinations (factually inaccurate or untruthful content relative
    to training data/input, also lack of faithfulness/groundedness) are a common AI
    failure.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Fraud risk; Health and safety risk; Legal risk; Operational risk;
    Reputational risk
  severity: High
  likelihood: Almost certain
- id: qube-mit-0591
  name: AI generated content unfairly represents
  description: Generated content might unfairly represent certain groups or individuals.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Legal risk; Reputational risk; Strategic risk
  severity: High
  likelihood: Likely
- id: qube-mit-0592
  name: AI decision bias unfairly advantages groups
  description: Decision bias occurs when a model unfairly advantages one group over
    another, potentially caused by data biases amplified during training.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Legal risk; Operational risk; Reputational risk
  severity: High
  likelihood: Likely
- id: qube-mit-0593
  name: AI reveals PII/SPI (data leakage)
  description: Models might reveal PII or SPI used in training, fine-tuning, or prompts
    (a type of data leakage).
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Data privacy risk; Legal risk; Reputational risk
  severity: High
  likelihood: Possible
- id: qube-mit-0594
  name: Terms, licenses restrict AI model use
  description: Terms of service, licenses, or other rules may restrict the use of
    certain models.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Legal risk; Operational risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0595
  name: AI responsibility hard without documentation
  description: Determining responsibility for an AI model is challenging without good
    documentation and governance.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Legal risk; Operational risk; Reputational risk
  severity: High
  likelihood: Likely
- id: qube-mit-0596
  name: Legal uncertainty AI content ownership
  description: Legal uncertainty exists regarding ownership and IP rights of AI-generated
    content.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Financial risk; Legal risk; Strategic risk
  severity: High
  likelihood: Likely
- id: qube-mit-0597
  name: Insufficient AI system documentation risks
  description: Insufficient documentation of the system using an AI model and the
    model's purpose within that system creates risks.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Operational risk; Reputational risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0598
  name: Unrepresentative AI testing, unreliable evaluation
  description: Unrepresentative testing occurs when test inputs don't match expected
    deployment inputs, leading to unreliable evaluation.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Operational risk; Reputational risk; Technological risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0599
  name: AI foundation model use changes risks
  description: A foundation model's intended use is crucial for defining its risks;
    as use changes, relevant risks may change.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Operational risk; Strategic risk
  severity: Medium
  likelihood: Almost certain
- id: qube-mit-0600
  name: AI data opacity hinders risk assessment
  description: Lack of data transparency from insufficient documentation of training/tuning
    dataset details hinders risk assessment.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Data privacy risk; Operational risk; Reputational
    risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0601
  name: Incorrect AI risk metric, flawed management
  description: An incorrectly selected or incomplete metric for tracking a risk, or
    measuring the wrong risk for a given context, leads to flawed risk management.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Operational risk; Strategic risk
  severity: Medium
  likelihood: Possible
- id: qube-mit-0602
  name: AI model opacity hinders understanding, trust
  description: Lack of model transparency from insufficient documentation of design,
    development, evaluation, and absence of insights into inner workings, hinders
    understanding and trust.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Operational risk; Reputational risk; Technological
    risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0603
  name: Socio-technical AI risks need diverse input
  description: AI model risks being socio-technical require broad disciplinary input
    and diverse testing practices for effective management.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Operational risk; Strategic risk; Technological risk
  severity: High
  likelihood: Likely
- id: qube-mit-0604
  name: AI overrepresents cultures, homogenization
  description: AI systems might overly represent certain cultures, leading to cultural
    homogenization and loss of diversity.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Reputational risk; Strategic risk
  severity: Medium
  likelihood: Possible
- id: qube-mit-0605
  name: GenAI access leads to student plagiarism
  description: Easy access to high-quality generative models may lead to students
    intentionally or unintentionally plagiarizing existing work.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Fraud risk; Legal risk; Reputational risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0606
  name: AI adoption leads to job losses
  description: Widespread adoption of foundation model-based AI systems might lead
    to job losses if workers are not reskilled for automated tasks.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Financial risk; Human resources risk; Strategic risk
  severity: High
  likelihood: Likely
- id: qube-mit-0607
  name: Excluding community perspectives hinders trust
  description: Failing to include perspectives of communities affected by model outcomes
    hinders understanding of relevant context and trust-building.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Reputational risk; Strategic risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0608
  name: GenAI access bypasses student learning
  description: Easy access to high-quality generative models might result in students
    using AI to bypass the learning process.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Human resources risk; Reputational risk; Strategic risk
  severity: Medium
  likelihood: Likely
- id: qube-mit-0609
  name: Large GenAI increases emissions, water use
  description: AI, particularly large generative models, might increase carbon emissions
    and water usage for training and operation, causing environmental harm.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Environmental risk; Financial risk; Reputational risk
  severity: High
  likelihood: Likely
- id: qube-mit-0610
  name: Poor AI worker conditions ethical risk
  description: Inadequate working conditions, unfair compensation, or poor healthcare
    (including mental health) for workers training AI models (e.g., ghost workers)
    is an ethical risk.
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Compliance risk; Health and safety risk; Human resources risk; Legal
    risk; Reputational risk; Third-party/vendor risk
  severity: Medium
  likelihood: Possible
- id: qube-mit-0611
  name: AI negatively affects individual autonomy
  description: AI might negatively affect individuals' ability to make choices and
    act independently in their best interests (loss of autonomy).
  isDefinedByTaxonomy: qube-legacy-mit
  tag: mit
  riskCategory: Health and safety risk; Reputational risk; Strategic risk
  severity: Medium
  likelihood: Possible
